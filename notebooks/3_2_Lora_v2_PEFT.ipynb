{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Development Environment"
      ],
      "metadata": {
        "id": "jRnIUyREwfzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvkunuwtoHvH",
        "outputId": "4dfab344-7371-409f-c526-b8275c3182c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbFO1UfZ1apU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c0f1986-24d7-466e-e183-8e7e1b2bccdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing CUDA device.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade git+https://github.com/huggingface/transformers\n",
        "!pip install -q --upgrade git+https://github.com/huggingface/peft.git\n",
        "!pip install -q bitsandbytes accelerate datasets tensorboardX loralib\n",
        "\n",
        "# Purpose of notebook: fine-tune LongT5 on exctracted sentences from studies, but using LoRA and bitsandbytes quantization\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from pprint import pprint\n",
        "import gc\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    LongT5ForConditionalGeneration,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS device.\")\n",
        "    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = \"0.0\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA device.\")\n",
        "    max_split_size_mb = 256  # Set the max_split_size_mb value (e.g., 512 MB)\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = f\"max_split_size_mb:{max_split_size_mb}\"\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"MPS/CUDA not available. Using CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configuration"
      ],
      "metadata": {
        "id": "JWbWlMwlYrkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- START CONFIG ----------\n",
        "# Load tokenizer and model\n",
        "model_id = 'pszemraj/long-t5-tglobal-base-16384-book-summary'\n",
        "output_dir = \"/drive/MyDrive/lora3/training_history\"  # Colab\n",
        "\n",
        "extracted_file_path = '/drive/MyDrive/lora3/biobert_extractive_only_training_dataset.csv.gz'  # Colab\n",
        "\n",
        "# source_data_path = \"data\"\n",
        "source_data_path = \"/drive/MyDrive/lora3/data\"  # Colab\n",
        "\n",
        "# longT5 max token length is 16384, let's 1/2 that\n",
        "max_input_token_length = 8192\n",
        "# max_input_token_length = 1024\n",
        "\n",
        "# -------- END CONFIG ----------"
      ],
      "metadata": {
        "id": "Tk1XOl10lYWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# label_pad_token_id = tokenizer.pad_token_id\n",
        "label_pad_token_id = -100  # special label token that gets ignored in loss calculations\n",
        "\n",
        "train_data_path = os.path.join(source_data_path, 'train_tokenized_dataset')\n",
        "val_data_path = os.path.join(source_data_path, 'val_tokenized_dataset')\n",
        "\n",
        "if os.path.exists(train_data_path) and os.path.exists(val_data_path):\n",
        "    train_dataset = Dataset.load_from_disk(train_data_path)\n",
        "    val_dataset = Dataset.load_from_disk(val_data_path)\n",
        "\n",
        "else:\n",
        "    ms2_dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split=\"train\")\n",
        "\n",
        "    # Load your CSV file\n",
        "    df = pd.read_csv(extracted_file_path, compression='gzip')\n",
        "\n",
        "    # # ---- if full extracted data is not available yet:\n",
        "    # all_extracted_summaries = []\n",
        "    # for fpath in os.listdir('../experiment_1/biobert_extractive_only_training_dataset'):\n",
        "    #     all_extracted_summaries.append(\n",
        "    #         pickle.load(open(os.path.join('../experiment_1/biobert_extractive_only_training_dataset', fpath), 'rb'))\n",
        "    #     )\n",
        "    # df = pd.DataFrame(all_extracted_summaries)\n",
        "    # # ----\n",
        "\n",
        "    target_texts = ms2_dataset['target']\n",
        "    input_texts = [\n",
        "        df[df['review_id'] == int(i)]['summary'].tolist()[0] for i in ms2_dataset['review_id']\n",
        "    ]\n",
        "    dataset = Dataset.from_dict({'input_text': input_texts, 'target_text': target_texts})\n",
        "\n",
        "    # Tokenize data\n",
        "    def tokenize_function(examples):\n",
        "        model_inputs = tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=max_input_token_length)\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            labels = tokenizer(text_target=examples['target_text'], padding='max_length', truncation=True, max_length=256)\n",
        "            labels[\"input_ids\"] = [\n",
        "                [(l if l != tokenizer.pad_token_id else label_pad_token_id) for l in label] for label in labels[\"input_ids\"]\n",
        "            ]\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        return model_inputs\n",
        "\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"input_text\", \"target_text\"])\n",
        "    print(f\"Keys of tokenized dataset: {list(tokenized_datasets.features)}\")\n",
        "\n",
        "    # Split the dataset\n",
        "    shuffle_dataset = tokenized_datasets.shuffle(seed=42)\n",
        "    shuffle_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    train_dataset = shuffle_dataset.select(range(len(tokenized_datasets) * 8 // 10))\n",
        "    val_dataset = shuffle_dataset.select(range(len(tokenized_datasets) * 8 // 10, len(tokenized_datasets)))\n",
        "\n",
        "    # save to disk for easy loading\n",
        "    train_dataset.save_to_disk(train_data_path)\n",
        "    val_dataset.save_to_disk(val_data_path)\n",
        "\n",
        "print(train_dataset[\"input_ids\"].shape)\n",
        "print(val_dataset[\"input_ids\"].shape)\n",
        "type(train_dataset[\"input_ids\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIN2cxxkl4Nn",
        "outputId": "88dbed49-491c-456e-97f1-08180f16e616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/spiece.model\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/tokenizer_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11350, 8192])\n",
            "torch.Size([2838, 8192])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ANALYSIS: what's the distribution of non-padding tokens in train_dataset[\"input_ids\"]?\n",
        "all_tokens = train_dataset[\"input_ids\"].numpy()\n",
        "non_pad_token_counts = np.array([len(np.where(tokens != 0)[0]) for tokens in all_tokens])\n",
        "# distribution of non_pad_token_counts\n",
        "display(pd.Series(non_pad_token_counts).describe())\n",
        "\n",
        "# what's the 95% percentile?\n",
        "print(\"95% percentile is\", np.percentile(non_pad_token_counts, 95))\n",
        "\n",
        "# which percentile is \"8192 non-padding tokens\" on?\n",
        "print(\n",
        "    \"If we truncated input_ids to 8192, this is the percentile it'll be at (anything at a higher percentile could risk losing information):\",\n",
        "    (perc_8192 := pd.Series(non_pad_token_counts).rank(pct=True)[np.where(non_pad_token_counts <= 8192)[0]].max())\n",
        ")\n",
        "# confirm\n",
        "print(np.percentile(non_pad_token_counts, perc_8192 * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "4wJ9pf-nqDr5",
        "outputId": "aa9d9d46-cb4b-4bad-ec96-919870083ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count    11350.000000\n",
              "mean      3661.722291\n",
              "std       2308.642882\n",
              "min         71.000000\n",
              "25%       1855.000000\n",
              "50%       3050.000000\n",
              "75%       5047.000000\n",
              "max       8192.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% percentile is 8192.0\n",
            "If we truncated input_ids to 8192, this is the percentile it'll be at (anything at a higher percentile could risk losing information): 0.9473568281938326\n",
            "8192.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bitsandbytes\n",
        "# Source notebooks:\n",
        "# - https://colab.research.google.com/drive/1Vvju5kOyBsDr7RX_YAvp6ZsSOoSMjhKD?usp=sharing#scrollTo=E0Nl5mWL0k2T\n",
        "# - https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing#scrollTo=HOWcL0LU3JYt\n",
        "# More background info:\n",
        "# - https://huggingface.co/blog/hf-bitsandbytes-integration\n",
        "\n",
        "checkpoint_path = \"longt5-qlora\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    # load_in_8bit=True,\n",
        ")\n",
        "\n",
        "base_model = LongT5ForConditionalGeneration.from_pretrained(model_id)\n",
        "model = LongT5ForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    # quantization_config=bnb_config,  # enable when in CUDA\n",
        "    # device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# # BUG: `model` has its embeddings reinitiated. Copy over from `base_model` but retain data type\n",
        "# reinited_params = ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
        "# for param_name in reinited_params:\n",
        "#     model_param = model.get_parameter(param_name)\n",
        "#     base_model_param = base_model.get_parameter(param_name)\n",
        "#     model_param.data = (\n",
        "#         base_model_param.data\n",
        "#         .to(model_param.dtype)  # or, comment out to remain in 32-bit for accuracy\n",
        "#         .to(device)\n",
        "#     )\n",
        "\n",
        "# use PEFT LoRA\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    # target_modules=[\"q\", \"v\", \"k\"],\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    # target_modules=[\"q\"],\n",
        "    layers_to_transform=list(range(0, 12)),  # 11 is max layer\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "# model = prepare_model_for_kbit_training(model)  # enable for 4bit or 8bit quantization\n",
        "model.enable_input_require_grads()\n",
        "model = get_peft_model(model, lora_config)\n",
        "# Fix from this GitHub issue: https://github.com/huggingface/peft/issues/522#issuecomment-1705989330\n",
        "model.base_model.model.encoder.enable_input_require_grads()\n",
        "model.base_model.model.decoder.enable_input_require_grads()\n",
        "\n",
        "model.train()\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Training arguments\n",
        "logpath = os.path.join(output_dir, checkpoint_path, \"logs\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=os.path.join(output_dir, checkpoint_path),\n",
        "    evaluation_strategy=\"steps\",  # alternatively, \"epoch\"\n",
        "    logging_strategy=\"steps\",\n",
        "    learning_rate=1e-3,\n",
        "    logging_dir=logpath,\n",
        "    report_to=\"tensorboard\",\n",
        "    save_strategy=\"steps\",\n",
        "    fp16=False,\n",
        "    # predict_with_generate=True,\n",
        "\n",
        "    # FOR REAL TRAINING\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # auto_find_batch_size=True,\n",
        "    eval_steps=200,\n",
        "    logging_steps=100,\n",
        "    save_steps=100,\n",
        "    log_level=\"info\",\n",
        "\n",
        "    # FOR DEBUGGING\n",
        "    # num_train_epochs=1,\n",
        "    # per_device_train_batch_size=1,\n",
        "    # per_device_eval_batch_size=1,\n",
        "    # max_steps=20,\n",
        "    # eval_steps=2,\n",
        "    # logging_steps=2,  # should match eval_steps\n",
        "    # save_steps=4,  # includes train loss metric\n",
        "    # log_level=\"debug\",\n",
        "\n",
        "    # FOR 4BIT OR 8BIT QUANTIZATION\n",
        "    # fp16=True,\n",
        "    # optim=\"paged_adamw_8bit\",  # default: adamw_torch\n",
        ")\n",
        "\n",
        "print(\"Tensorboard log path:\", logpath)\n",
        "print(\"run this in terminal: tensorboard --logdir\", logpath)\n",
        "\n",
        "# Initialize Trainer\n",
        "model.config.use_cache = False\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    # model=model_id,\n",
        "    # label_pad_token_id=label_pad_token_id,\n",
        "    # pad_to_multiple_of=8,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset.shuffle(seed=42).select(range(200)),\n",
        "    # eval_dataset=val_dataset.select(range(10, 20)),  # for debugging\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0rfUSQNqTjZ",
        "outputId": "02d4fe2f-6098-4295-98b0-5fe15cc827b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing LongT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of LongT5ForConditionalGeneration were initialized from the model checkpoint at pszemraj/long-t5-tglobal-base-16384-book-summary.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongT5ForConditionalGeneration for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing LongT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of LongT5ForConditionalGeneration were initialized from the model checkpoint at pszemraj/long-t5-tglobal-base-16384-book-summary.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongT5ForConditionalGeneration for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "PyTorch: setting up devices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,769,472 || all params: 249,356,928 || trainable%: 0.7096141319161583\n",
            "Tensorboard log path: /drive/MyDrive/lora3/training_history/longt5-qlora/logs\n",
            "run this in terminal: tensorboard --logdir /drive/MyDrive/lora3/training_history/longt5-qlora/logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Weights"
      ],
      "metadata": {
        "id": "zXRQuTB9ZFCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.get_parameter(\"encoder.embed_tokens.weight\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFmMeDWlwQW4",
        "outputId": "d5caf073-89f7-4e83-e94e-eb944108458b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.5561,  0.4233,  0.8544,  ..., -0.9618,  0.6647,  0.9398],\n",
              "        [ 0.4269,  1.6681,  4.5766,  ..., -2.2274, -0.5151,  2.1782],\n",
              "        [-5.4195, -2.4177, -0.8740,  ..., -0.2788, -1.3139, -1.5880],\n",
              "        ...,\n",
              "        [ 1.5533,  0.5635,  1.6218,  ...,  1.9036,  0.7348,  0.1447],\n",
              "        [ 0.2494,  0.8528, -0.6396,  ...,  0.1166, -1.1269,  0.8604],\n",
              "        [ 0.8795, -0.3369, -1.7056,  ...,  0.4987,  1.2487,  0.6472]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_parameter(\"encoder.embed_tokens.weight\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_Y8vPL8wRl3",
        "outputId": "f419c1e5-6312-47ad-c284-cf377326db2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.5561,  0.4233,  0.8544,  ..., -0.9618,  0.6647,  0.9398],\n",
              "        [ 0.4269,  1.6681,  4.5766,  ..., -2.2274, -0.5151,  2.1782],\n",
              "        [-5.4195, -2.4177, -0.8740,  ..., -0.2788, -1.3139, -1.5880],\n",
              "        ...,\n",
              "        [ 1.5533,  0.5635,  1.6218,  ...,  1.9036,  0.7348,  0.1447],\n",
              "        [ 0.2494,  0.8528, -0.6396,  ...,  0.1166, -1.1269,  0.8604],\n",
              "        [ 0.8795, -0.3369, -1.7056,  ...,  0.4987,  1.2487,  0.6472]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\").shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RngaMLOFwfFw",
        "outputId": "f0c1876b-b266-496a-a597-4255561df711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0101, -0.0265, -0.0220,  ..., -0.0213,  0.0272,  0.0029],\n",
            "        [ 0.0101,  0.0343, -0.0018,  ..., -0.0310, -0.0269,  0.0170],\n",
            "        [-0.0231,  0.0304, -0.0076,  ..., -0.0079,  0.0180,  0.0260],\n",
            "        ...,\n",
            "        [-0.0268,  0.0280,  0.0079,  ..., -0.0042,  0.0326,  0.0029],\n",
            "        [ 0.0194, -0.0282, -0.0063,  ..., -0.0314, -0.0312,  0.0103],\n",
            "        [-0.0235,  0.0159, -0.0090,  ...,  0.0287,  0.0335, -0.0317]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([16, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\").shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPP9SrMCwkTa",
        "outputId": "7d543b2f-2319-47e9-f926-fb0783400b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([768, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[\"labels\"].device)\n",
        "model.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igjr6HD2wnQf",
        "outputId": "a23bfa32-acdc-4159-bb8d-9f65fde9d2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "base_model = base_model.to(device)\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = base_model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128, num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e55XuAairRT3",
        "outputId": "f6fd85ed-f0b2-49f6-8cb6-d7e6c0e29112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('The aim of this study is to describe the safety and efficacy of a local '\n",
            " 'analgese in patients suffering from hypertensives. All patients presented '\n",
            " 'with prehypertension, stage1 hypertension, and stage2 hypertension. There '\n",
            " 'were no significant differences in blood pressure between the groups except '\n",
            " 'for those with elevated pulse rate. This study was performed at Shaikhzayed '\n",
            " 'Medical Complex on May to December 2008. Out of these sixty patients, 10 '\n",
            " 'have pre-hyptertension, 10 hadstage 1 hypertension; 10 had stage 2, '\n",
            " 'hypertension where it slightly increased. Mean Pul speed increased from 3 to '\n",
            " '4 beat')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = trainer.model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128, num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr2C4Iupv5qR",
        "outputId": "d1b4fb24-6a59-4a7c-eb5d-bb8a3a8ef164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1202: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('In this paper, we describe a method to assess the safety of two different '\n",
            " 'types of local analization in patients with hypertension. We hypothesizes '\n",
            " 'how fast blood pressure changes and pulse rate change after tooth extraction '\n",
            " 'using a combination of 2 g/mL of ligonine and 1 mmoll of epitomephrine. This '\n",
            " 'is sufficient information for us to conclude that these changes are not due '\n",
            " 'to anti-hyperpensive drugs.')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "call_outputs = model(\n",
        "    inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    labels=train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id].unsqueeze(0).to(device),\n",
        ")\n",
        "print(\"Without padding tokens\")\n",
        "print(call_outputs.loss)\n",
        "print(call_outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uCWWXDjwwMG",
        "outputId": "dd12536f-be23-469f-c301-6d9ce6034516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without padding tokens\n",
            "tensor(3.9818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor([[[-19.3257,  -5.1725,  -5.3487,  ..., -19.1243, -19.4162, -19.3571],\n",
            "         [-27.8897,  -6.3095,  -4.4837,  ..., -27.2778, -27.9679, -27.6794],\n",
            "         [-33.3910,  -6.4139, -11.6583,  ..., -32.8830, -33.3757, -33.1600],\n",
            "         ...,\n",
            "         [-27.2299,  -2.6808,  -6.7620,  ..., -26.6056, -27.2207, -27.2371],\n",
            "         [-33.2471,  -5.8694,  -7.2000,  ..., -32.6066, -33.2561, -33.2425],\n",
            "         [-28.5755,   1.7134,  -6.0601,  ..., -27.8876, -28.5938, -28.4779]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "call_outputs = model(\n",
        "    inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    labels=train_dataset[id_to_choose]['labels'].unsqueeze(0).to(device),\n",
        ")\n",
        "print(\"With padding tokens in labels\")\n",
        "print(call_outputs.loss)\n",
        "print(call_outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hjvBU9Vw2Xi",
        "outputId": "9fc9c13b-34b5-45cd-ecc8-e1e48e00e346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With padding tokens in labels\n",
            "tensor(3.5110, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor([[[-17.5235,  -3.7862,  -5.2687,  ..., -17.2704, -17.4914, -17.4873],\n",
            "         [-22.2564,  -4.6774,  -3.7386,  ..., -21.7536, -22.1952, -22.0019],\n",
            "         [-24.6808,  -9.0822,  -8.0153,  ..., -24.3900, -24.6045, -24.5217],\n",
            "         ...,\n",
            "         [-19.1310,  -2.8402,  -4.2411,  ..., -18.7630, -19.0941, -19.0569],\n",
            "         [-18.7427,  -2.2456,  -4.7864,  ..., -18.4570, -18.7313, -18.6435],\n",
            "         [-17.9228,  -1.5371,  -4.3734,  ..., -17.5935, -17.9750, -17.8579]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in trainer.model.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsmxcs4aw51F",
        "outputId": "fd2101f3-c37f-4436-b4c6-c57f4b42631c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model.model.shared.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.relative_attention_bias.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.global_relative_attention_bias.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.0.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.0.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.1.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.1.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.2.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.2.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.3.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.3.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.4.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.4.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.5.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.5.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.6.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.6.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.7.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.7.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.8.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.8.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.8.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.9.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.9.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.9.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.10.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.10.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.10.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.11.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.11.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.11.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.final_layer_norm.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\n",
            "base_model.model.decoder.block.0.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.0.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.0.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.1.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.1.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.1.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.2.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.2.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.2.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.3.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.3.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.3.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.4.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.4.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.4.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.5.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.5.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.5.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.6.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.6.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.6.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.7.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.7.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.7.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.8.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.8.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.8.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.8.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.9.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.9.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.9.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.9.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.10.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.10.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.10.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.10.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.11.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.11.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.11.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.11.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.final_layer_norm.weight False\n",
            "base_model.model.lm_head.weight False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z3uNGRixAKd",
        "outputId": "94c15380-94a6-43f4-9e24-2cbfc3a1eddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSeq2SeqLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LongT5ForConditionalGeneration(\n",
              "      (shared): Embedding(32128, 768)\n",
              "      (encoder): LongT5Stack(\n",
              "        (embed_tokens): Embedding(32128, 768)\n",
              "        (block): ModuleList(\n",
              "          (0): LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerTransientGlobalSelfAttention(\n",
              "                (TransientGlobalSelfAttention): LongT5TransientGlobalAttention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 12)\n",
              "                  (global_relative_attention_bias): Embedding(32, 12)\n",
              "                  (global_input_layer_norm): LongT5LayerNorm()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-11): 11 x LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerTransientGlobalSelfAttention(\n",
              "                (TransientGlobalSelfAttention): LongT5TransientGlobalAttention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (global_input_layer_norm): LongT5LayerNorm()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): LongT5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (decoder): LongT5Stack(\n",
              "        (embed_tokens): Embedding(32128, 768)\n",
              "        (block): ModuleList(\n",
              "          (0): LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerSelfAttention(\n",
              "                (SelfAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 12)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerCrossAttention(\n",
              "                (EncDecAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-11): 11 x LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerSelfAttention(\n",
              "                (SelfAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerCrossAttention(\n",
              "                (EncDecAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): LongT5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "wV1q_ZSUZVNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (If needed) Load model from checkpoint\n",
        "latest_checkpoint = max([int(f.split('-')[1]) for f in os.listdir(os.path.join(output_dir, checkpoint_path)) if f.startswith('checkpoint')])\n",
        "if latest_checkpoint:\n",
        "    resume_from_checkpoint = os.path.join(output_dir, checkpoint_path, f\"checkpoint-{latest_checkpoint}\")\n",
        "    print(\"Resuming from checkpoint:\", resume_from_checkpoint)\n",
        "else:\n",
        "    resume_from_checkpoint = None"
      ],
      "metadata": {
        "id": "WdUTlUJgxBKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(resume_from_checkpoint=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QLe8jPczxRCA",
        "outputId": "e00d4288-8ac0-4715-e157-6a3c52edb370"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 11,350\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5,676\n",
            "  Number of trainable parameters = 1,769,472\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4196' max='5676' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4196/5676 7:38:43 < 2:41:52, 0.15 it/s, Epoch 2.96/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.061600</td>\n",
              "      <td>2.658275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.996600</td>\n",
              "      <td>2.594736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.952400</td>\n",
              "      <td>2.577842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.979000</td>\n",
              "      <td>2.566224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.868200</td>\n",
              "      <td>2.546876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.853500</td>\n",
              "      <td>2.537159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>2.861100</td>\n",
              "      <td>2.526041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>2.813400</td>\n",
              "      <td>2.522116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>2.824200</td>\n",
              "      <td>2.510067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.855900</td>\n",
              "      <td>2.510564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>2.773400</td>\n",
              "      <td>2.490865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>2.861600</td>\n",
              "      <td>2.501967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>2.829000</td>\n",
              "      <td>2.493399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>2.821100</td>\n",
              "      <td>2.498792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.719600</td>\n",
              "      <td>2.485958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>2.750200</td>\n",
              "      <td>2.481304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>2.760900</td>\n",
              "      <td>2.477468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>2.736400</td>\n",
              "      <td>2.477658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>2.728200</td>\n",
              "      <td>2.472629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.756400</td>\n",
              "      <td>2.466641</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-100\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-100/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-100/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-200\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-200/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-200/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-300\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-300/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-300/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-300/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-400\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-400/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-400/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-400/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-500/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-500/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-600\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-600/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-600/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-600/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-700\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-700/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-700/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-700/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-800\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-800/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-800/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-800/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-900\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-900/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-900/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-900/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1000/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1000/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1100\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1100/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1100/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1100/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1200\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1200/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1200/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1200/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1300\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1300/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1300/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1300/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1400\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1400/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1400/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1400/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1500/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1500/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1600\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1600/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1600/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1600/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1700\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1700/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1700/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1700/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1800\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1800/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1800/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1800/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1900\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1900/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1900/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-1900/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2000/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2000/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2100\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2100/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2100/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2100/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2200\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2200/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2200/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2200/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2300\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2300/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2300/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2300/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2400\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2400/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2400/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2400/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2500/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2500/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2600\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2600/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2600/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2600/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2700\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2700/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2700/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2700/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2800\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2800/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2800/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2800/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2900\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2900/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2900/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-2900/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3000/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3000/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3100\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3100/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3100/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3100/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3200\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3200/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3200/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3200/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3300\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3300/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3300/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3300/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3400\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3400/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3400/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3400/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3500/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3500/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3600\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3600/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3600/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3600/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3700\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3700/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3700/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3700/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3800\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3800/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3800/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3800/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3900\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3900/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3900/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-3900/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4000/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4000/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4100\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4100/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4100/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4100/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5676' max='5676' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5676/5676 10:20:59, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.061600</td>\n",
              "      <td>2.658275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.996600</td>\n",
              "      <td>2.594736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.952400</td>\n",
              "      <td>2.577842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.979000</td>\n",
              "      <td>2.566224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.868200</td>\n",
              "      <td>2.546876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.853500</td>\n",
              "      <td>2.537159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>2.861100</td>\n",
              "      <td>2.526041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>2.813400</td>\n",
              "      <td>2.522116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>2.824200</td>\n",
              "      <td>2.510067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.855900</td>\n",
              "      <td>2.510564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>2.773400</td>\n",
              "      <td>2.490865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>2.861600</td>\n",
              "      <td>2.501967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>2.829000</td>\n",
              "      <td>2.493399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>2.821100</td>\n",
              "      <td>2.498792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.719600</td>\n",
              "      <td>2.485958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>2.750200</td>\n",
              "      <td>2.481304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>2.760900</td>\n",
              "      <td>2.477468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>2.736400</td>\n",
              "      <td>2.477658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>2.728200</td>\n",
              "      <td>2.472629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.756400</td>\n",
              "      <td>2.466641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>2.737400</td>\n",
              "      <td>2.464423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>2.690500</td>\n",
              "      <td>2.465390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>2.701700</td>\n",
              "      <td>2.462291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>2.665300</td>\n",
              "      <td>2.460503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.726600</td>\n",
              "      <td>2.458219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>2.713700</td>\n",
              "      <td>2.457783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>2.682000</td>\n",
              "      <td>2.456538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>2.707000</td>\n",
              "      <td>2.453746</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4200\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4200/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4200/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4200/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4300\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4300/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4300/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4300/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4400\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4400/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4400/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4400/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4500/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4500/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4600\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4600/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4600/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4600/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4700\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4700/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4700/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4700/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4800\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4800/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4800/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4800/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4900\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4900/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4900/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-4900/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5000/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5000/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5100\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5100/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5100/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5100/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5200\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5200/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5200/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5200/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5300\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5300/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5300/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5300/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5400\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5400/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5400/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5400/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5500/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5500/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5600\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5600/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5600/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora/checkpoint-5600/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5676, training_loss=2.7999307741322426, metrics={'train_runtime': 37266.9698, 'train_samples_per_second': 1.218, 'train_steps_per_second': 0.152, 'total_flos': 5.013768845131776e+17, 'train_loss': 2.7999307741322426, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model"
      ],
      "metadata": {
        "id": "bDZumyMVZZLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on custom slice of train dataset\n",
        "trainer.evaluate(train_dataset.select(range(0, 10)))"
      ],
      "metadata": {
        "id": "9_SIlfUlxuK9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "b7997773-9b71-445a-b166-c476c9aff4f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 2.4203221797943115,\n",
              " 'eval_runtime': 2.2725,\n",
              " 'eval_samples_per_second': 4.4,\n",
              " 'eval_steps_per_second': 0.88,\n",
              " 'epoch': 4.0}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# view results\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "9aBvbHBUx21Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "c96d464b-ea03-4b35-e2d1-7d536e569166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:43]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 2.4536397457122803,\n",
              " 'eval_runtime': 42.7509,\n",
              " 'eval_samples_per_second': 4.678,\n",
              " 'eval_steps_per_second': 0.585,\n",
              " 'epoch': 4.0}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\").shape)"
      ],
      "metadata": {
        "id": "3L8yUl3Fx9Lf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e97b001-98a8-4274-fc8d-b71059651578"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0013,  0.0397, -0.0375,  ...,  0.0296,  0.0266, -0.0718],\n",
            "        [-0.1112,  0.0887, -0.0115,  ..., -0.0763, -0.0597,  0.0898],\n",
            "        [ 0.0289,  0.0019,  0.0370,  ..., -0.0213,  0.0014,  0.0523],\n",
            "        ...,\n",
            "        [-0.0602,  0.0623,  0.0565,  ..., -0.0050, -0.0475,  0.0181],\n",
            "        [-0.1165, -0.0449, -0.0347,  ..., -0.0963, -0.0890,  0.0524],\n",
            "        [ 0.0728,  0.0018,  0.0158,  ...,  0.0616,  0.0553, -0.0699]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([16, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\").shape)"
      ],
      "metadata": {
        "id": "t5eh0vSdx-GR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99814b8-a1a7-4160-afce-e85f774150ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0457,  0.0409,  0.0206,  ..., -0.0350,  0.0114,  0.0443],\n",
            "        [ 0.0236,  0.0163,  0.0056,  ...,  0.0046, -0.0030,  0.0189],\n",
            "        [-0.0067,  0.0025,  0.0173,  ..., -0.0040, -0.0092,  0.0102],\n",
            "        ...,\n",
            "        [-0.0105, -0.0284,  0.0229,  ..., -0.0418, -0.0465, -0.0166],\n",
            "        [-0.0003,  0.0033,  0.0088,  ...,  0.0819,  0.0046, -0.0212],\n",
            "        [ 0.0027,  0.0263, -0.0178,  ..., -0.0667, -0.0055,  0.0349]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([768, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = trainer.model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128,\n",
        "    num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "1eQYtMRAyCRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32194dad-846b-4470-e1c9-aff6b65028f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('There was no statistically significant difference in blood pressure or pulse '\n",
            " 'rate between the groups. Conclusions This meta- analysis suggests that there '\n",
            " 'is no evidence to support the use of local anesthesia for restorative '\n",
            " 'dentistry')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = trainer.model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128,\n",
        "    num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "icvCoL33yeX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc374fcb-33e9-4e91-efe1-fab5b521623a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('There was no statistically significant difference in blood pressure or pulse '\n",
            " 'rate between the groups. Conclusions This meta- analysis suggests that there '\n",
            " 'is no evidence to support the use of local anesthesia for restorative '\n",
            " 'dentistry')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "call_outputs = model(\n",
        "    inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    labels=train_dataset[id_to_choose]['labels'].unsqueeze(0).to(device),\n",
        ")\n",
        "print(\"With padding tokens in labels\")\n",
        "print(call_outputs.loss)\n",
        "print(call_outputs.logits)"
      ],
      "metadata": {
        "id": "wtWNgZNFygEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ec9bf6-8db4-4442-ab93-3ec622ed5608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With padding tokens in labels\n",
            "tensor(2.3746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor([[[-14.7351,   0.4726,  -1.4425,  ..., -14.4264, -14.7627, -14.7134],\n",
            "         [-17.2709,  -4.1753,  -3.2428,  ..., -16.9747, -17.3637, -17.1941],\n",
            "         [-17.4424,  -4.9862,  -6.3705,  ..., -17.1304, -17.4767, -17.2945],\n",
            "         ...,\n",
            "         [-15.1513,   0.6501,  -1.4678,  ..., -14.8386, -15.1927, -15.1244],\n",
            "         [-15.1512,   0.6500,  -1.4676,  ..., -14.8385, -15.1926, -15.1242],\n",
            "         [-15.1510,   0.6498,  -1.4674,  ..., -14.8383, -15.1924, -15.1241]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "final_save_dir = \"longt5-qlora-4-epochs-final\"\n",
        "trainer.model.save_pretrained(os.path.join(output_dir, final_save_dir))\n",
        "tokenizer.save_pretrained(os.path.join(output_dir, final_save_dir))"
      ],
      "metadata": {
        "id": "C-HnpZYRyl7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a248667-40a1-4b2f-b4b9-a0a54bfde1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pszemraj--long-t5-tglobal-base-16384-book-summary/snapshots/8988ae13e60c84ba15e894a934c4364afceedab6/config.json\n",
            "Model config LongT5Config {\n",
            "  \"architectures\": [\n",
            "    \"LongT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_type\": \"transient-global\",\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"global_block_size\": 16,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"local_radius\": 127,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"model_type\": \"longt5\",\n",
            "  \"n_positions\": 4096,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /drive/MyDrive/lora3/training_history/longt5-qlora-4-epochs-final/tokenizer_config.json\n",
            "Special tokens file saved in /drive/MyDrive/lora3/training_history/longt5-qlora-4-epochs-final/special_tokens_map.json\n",
            "Copy vocab file to /drive/MyDrive/lora3/training_history/longt5-qlora-4-epochs-final/spiece.model\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/drive/MyDrive/lora3/training_history/longt5-qlora-4-epochs-final/tokenizer_config.json',\n",
              " '/drive/MyDrive/lora3/training_history/longt5-qlora-4-epochs-final/special_tokens_map.json',\n",
              " '/drive/MyDrive/lora3/training_history/longt5-qlora-4-epochs-final/spiece.model',\n",
              " '/drive/MyDrive/lora3/training_history/longt5-qlora-4-epochs-final/added_tokens.json',\n",
              " '/drive/MyDrive/lora3/training_history/longt5-qlora-4-epochs-final/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "label_pad_token_id = -100  # special label token that gets ignored in loss calculations\n",
        "\n",
        "test_data_path = os.path.join(source_data_path, 'test_tokenized_dataset')\n",
        "if os.path.exists(test_data_path):\n",
        "    test_dataset = Dataset.load_from_disk(test_data_path)\n",
        "else:\n",
        "    test_dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split=\"validation\")  # test set does not have target summaries\n",
        "    test_dataset.save_to_disk(test_data_path)\n",
        "\n",
        "# Load Kmeans extraction\n",
        "df_kmeans_extractive_test = pd.read_csv(\n",
        "    \"/drive/MyDrive/lora3/data/BioBERT_K_Means_extractive.csv\",\n",
        "    index_col=0,\n",
        "    dtype={'review_id': str, 'summary': str}\n",
        ")\n",
        "display(df_kmeans_extractive_test.head())\n",
        "\n",
        "# df_kmeans_extractive_test's summary gets appended as \"input_text\" in test_dataset, but in the same order as test_dataset\n",
        "input_text_ordered = [\n",
        "    df_kmeans_extractive_test[df_kmeans_extractive_test['review_id'] == rid]['summary'].tolist()[0]\n",
        "    for rid in test_dataset['review_id']\n",
        "]\n",
        "test_dataset = test_dataset.add_column('input_text', input_text_ordered)\n",
        "\n",
        "# rename \"target\" to \"target_text\" to match training dataset\n",
        "test_dataset = test_dataset.rename_column('target', 'target_text')\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=max_input_token_length)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(text_target=examples['target_text'], padding='max_length', truncation=True, max_length=256)\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else label_pad_token_id) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, num_proc=4)\n",
        "tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "print(f\"Keys of tokenized dataset: {list(tokenized_test_dataset.features)}\")"
      ],
      "metadata": {
        "id": "z9rjrQY_yzIn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419,
          "referenced_widgets": [
            "7cd642a5727c4146ac475667867c607e",
            "e8a431b75ac8424f8734580e045e57aa",
            "0bd485803a00419ca0fd7efc5ff63bac",
            "d224f4fce71f4ebeb7c25f4617bac786",
            "0681e895ea874b3082d03538534ebdfe",
            "a85fa24b77be493fa1a3e670f974923c",
            "cf5df40390284c6a8148d57f94e6f5c0",
            "d747fbfee5f84a00a8adb523c86a9acc",
            "524772da150f4b1ca419ac5075f72ca9",
            "1a19041d8f604a688c1c5c7d60b4ed9f",
            "c502f48fdc2a496dbe6b268aa3b03bad"
          ]
        },
        "outputId": "af9d14a7-f19d-4525-d99b-68d88750c220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  review_id                                            summary\n",
              "0  28514886  Breast-fed infants typically have an intestina...\n",
              "1  18842808  No adverse effects were observed . The effects...\n",
              "2  24297836  Autonomic cardiovascular dysfunction accompani...\n",
              "3  32367221  Abstract . Pain on kneeling , KT-1000 measured...\n",
              "4  25038833  RESULTS Results of the Name-Face Association T..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2c9c5fb-1526-428e-b664-7f6fb818635b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28514886</td>\n",
              "      <td>Breast-fed infants typically have an intestina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18842808</td>\n",
              "      <td>No adverse effects were observed . The effects...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24297836</td>\n",
              "      <td>Autonomic cardiovascular dysfunction accompani...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32367221</td>\n",
              "      <td>Abstract . Pain on kneeling , KT-1000 measured...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25038833</td>\n",
              "      <td>RESULTS Results of the Name-Face Association T...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2c9c5fb-1526-428e-b664-7f6fb818635b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d2c9c5fb-1526-428e-b664-7f6fb818635b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d2c9c5fb-1526-428e-b664-7f6fb818635b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fa95de2c-1786-4287-b4df-d7fd7d5a70dd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fa95de2c-1786-4287-b4df-d7fd7d5a70dd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fa95de2c-1786-4287-b4df-d7fd7d5a70dd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(f\\\"Keys of tokenized dataset: {list(tokenized_test_dataset\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"review_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"18842808\",\n          \"25038833\",\n          \"24297836\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"No adverse effects were observed . The effects of the soluble fiber konjac glucomannan ( GM ) on serum cholesterol concentrations were investigated in 63 healthy men in a double-blind crossover , placebo-controlled study . After a 2-wk baseline period , the subjects were given 3.9 g GM or placebo daily for 4 wk . GM fibers reduced total cholesterol ( TC ) concentrations by 10 % ( P < 0.0001 ) , low-density-lipoprotein cholesterol ( LDL-C ) concentrations by 7.2 % ( P < 0.007 ) , triglycerides by 23 % ( P < 0.03 ) , and systolic blood pressure by 2.5 % ( P < 0.02 ) . No change in diastolic blood pressure or body weight was observed . BACKGROUND Fiber supplements added to a caloric diet have additional effects on weight reduction in overweight subjects . The fiber supplements consisted of the viscous fibers glucomannan ( Chrombalance ) , glucomannan and guar gum ( Appe-Trim ) and glucomannan , guar gum and alginat ( Glucosahl ) . The aim of this study was to compare the effect of various commercial fiber supplements ( glucomannan , guar gum and alginate ) on weight reduction in healthy overweight subjects . However , there were no significant differences between the different fibers in their ability to induce weight reduction , which was approximately 0.8 kg/week ( 3.8 + /- 0.9 , 4.4 + /- 2.0 , 4.1 + /- 0.6 in the Chrombalance , Appe-Trim and Glucosahl group , respectively ) . MATERIAL / METHODS One hundred and seventy six men and women were included to receive either active fiber substance or placebo in r and omized placebo-controlled studies . Dietary fibres are frequently used for the treatment of paediatric obesity . At the beginning of the study the drug and the placebo groups were comparable in regards to anthropometric data . ( ABSTRACT TRUNCATED AT 250 WORDS We suggest that this metabolic alteration may derive from a primary decrease of alpha-lipoprotein , most likely because of an inadequate water intake . This experimental design was double blinded with a block r and omisation , alpha = 0.05 , beta = 0.2 and delta = 50 % . Using a parallel-arm , double-blind , placebo-controlled design , 30 overweight and obese men ( body mass index , 25 - 35 kg/m(2 ) ) Conversely , LDL-C reductions were significant in the placebo group only after 12 weeks ( -6.0 % , P < .05 ) . Compliance was excellent as assessed by 7-day weighed dietary records and ketonuria . Both groups experienced decreases in ( P < .01 ) body weight , percent body fat , systolic blood pressure , waist circumference , and plasma glucose levels . The objective of the present study was to determine the effect of adding soluble fiber to a CRD on plasma LDL-C and other traditionally measured markers of cardiovascular disease . CONCLUSIONS These results suggest that glucomannan may represent a rationale adjunct to diet therapy in primary prevention in high risk hypercholesterolemic children After another 8 weeks of treatment , the results were compared within and between the two groups . METHODS All the subjects recruited underwent an 8-week run in diet period ; a Step-One-Diet was prescribed . The percentage decrease showed a statistically significant difference between sex groups . Decreases were observed in favor of female vs. male children in TC ( 24 % vs. 9 % ) and LDL-C ( 30 % vs. 9 % ) . To evaluate the effectiveness of highly purified glucomannan in childhood obesity a study has been carried out in 23 obese children ( 12 boys and 11 girls , aged 5.2 - 15.8 years ) , with excess weight of 51 + /- Excess weight and triglycerides levels were significantly decreased in treated obese patients than in obese controls 4 months after the beginning of the study . No important side-effects were observed in treated patients . After a three-days food recall , a balanced diet with adequate caloric intake was provided to all obese children . In all patients before and 2 - 4 months after the auxological data ( weight , height , weight excess ) and laboratory data ( serum levels of cholesterol , HDL , triglycerides , glucose , fructosamine , glycosylated hemoglobin , RBC , WBC , hemoglobin , iron , calcium , Cu and Zn ) have been determined . Plasma low-density lipoprotein ( LDL ) cholesterol concentrations were decreased ( P<0.05 ) after glucomannan ( 3.16\\u00b10.14 mmol/l ) and combination treatments ( 2.95\\u00b10.16 mmol/l ) compared to control ( 3.60\\u00b10.16 mmol/l ) . Design : Objective : The purpose of this study was to determine whether supplements of plant sterols and /or glucomannan improve lipid profile and cholesterol bio synthesis in mildly hypercholesterolemic type II diabetic and non-diabetic subjects and to compare the response of these two subject groups to the treatments . A r and omized , crossover study consisting of four phases of 21 days , with each phase separated by a 28-day washout . Setting : The Mary Emily Clinical Nutrition Research Unit of McGill University . Feces collected at the end of each experimental period were analyzed for neutral sterol and bile acid contents . Objectives : The present study was design ed to evaluate effects of konjac glucomannan ( KGM ) supplement ( 3.6 g/day ) for 28 days on blood lipid and glucose levels in hyperlipidemic type 2 diabetic patients and the possible mechanism for the reductions in blood lipid levels . Methods : Twenty-two diabetic subjects ( age 64.2 + 8.4 years , BMI 25.5 + 3.2 kg/m2 ) with elevated blood cholesterol levels ( fasting glucose between 6.7\\u201314.4 mmol/L ) , but currently not taking lipid-lowering medication , were recruited to participate in a two 28-day period , r and omized , double-blind , crossover clinical trial . Plasma triglyceride , HDL-cholesterol , LDL/HDL cholesterol , postpr and ial glucose and body weight were not significant after adjustment by the Bonferroni-Hochberg procedure . Results : Compared with placebo , KGM effectively reduced plasma cholesterol ( 11.1 % , p = 0.0001 , adjusted \\u03b1 = 0.006 ) , LDL-cholesterol ( 20.7 % , p = 0.0004 , adjusted \\u03b1 = 0.006 ) , total/HDL cholesterol ratio ( 15.6 % , p = 0.0005 , adjusted \\u03b1 = 0.007 ) , ApoB ( 12.9 % , p = 0.0001 , adjusted \\u03b1 = 0.006 ) and fasting glucose ( 23.2 % , p = 0.002 , adjusted \\u03b1 = 0.008 ) . An eight-week double-blind trial was conducted to test purified glucomannan fiber as a food supplement in 20 obese subjects . Glucomannan fiber ( from konjac root ) or placebo was given in 1-g doses ( two 500 mg capsules ) with 8 oz water , 1 h prior to each of three meals per d. Subjects were instructed not to change their eating or exercise patterns . Results showed a significant mean weight loss ( 5.5 lbs ) using glucomannan over an eight-week period . Serum cholesterol and low-density lipoprotein cholesterol were significantly reduced ( 21.7 and 15.0 mg/dl respectively ) in the glucomannan treated group . No adverse reactions to glucomannan were reported In type 2 diabetes , pre-pr and ial glucomannan ingestion attenuated a rise of blood glucose without significantly affecting insulin levels . CONCLUSION MATERIAL AND METHOD A single-blind , placebo-controlled , crossover trial with two treatments separated by a 2-week washout period was performed in 10 men and 10 women with type 2 diabetes mellitus . Two separated protocol s of experiments were sequentially followed . Glucomannan also decreased the rise of low-density lipoprotein cholesterol ( LDL-C ) . Two groups of 25 severely obese patients underwent 3 months of hypocaloric diet therapy either alone or associated with a glucomannan-based fibrous diet supplement ( approx . 4 g/die in 3 doses ) . The comparative analysis of the results obtained in both groups showed that the diet + glucomannan group had a more significant weight loss in relation to the fatty mass alone , an overall improvement in lipid status and carbohydrate tolerance , and a greater adherence to the diet in the absence of any relevant side effects . Due to the marked ability to satiate patients and the positive metabolic effects , glucomannan diet supplements have been found to be particularly efficacious and well tolerated even in the long-term treatment of severe obesity\",\n          \"RESULTS Results of the Name-Face Association Test at week 3 showed no significant difference between darifenacin and placebo on delayed recall ( mean difference , -0.06 , p=0.908 ) . METHODS Healthy subjects ( n=150 ) > No between-treatment differences were detected in self-rated memory , demonstrating that subjects were unaware of memory deterioration . CONCLUSIONS While darifenacin had no significant effects on memory versus placebo , oxybutynin ER caused significant memory deterioration ( magnitude of effect comparable to brain aging of 10 years ) . /=60 years were r and omised to darifenacin , oxybutynin ER or placebo in a multicentre , double-blind , double-dummy , parallel-group , 3-week study . There was no significant association between anti-cholinergic score and time spent as an inpatient ( adjusted for survival time ) DISCUSSION ( p = 0.94 ) ; or survival time . days before death ( survival ) , with mean of 4.8 ( median 3 , SD 4.18 , range 1 - 24 ) study assessment s in this time period . INTRODUCTION Anti-cholinergic medications have been associated with increased risks of cognitive impairment , premature mortality and increased risk of hospitalisation . RESULTS Eighty-seven patients were included . CONCLUSION Pharmacist-initiated drug changes significantly reduced ADS score but did not improve cognitive function  in nursing home residents . The median ADS score was reduced by 2 units ( p < .0001 ) in the intervention group and remained unchanged in the control group . Secondary end points were Mini-Mental Sate Examination , delayed recall and recognition of words , saliva flow , and serum anticholinergic activity (SAA).The participants were retested after 4 and 8 weeks , and the study groups were compared after adjusting for baseline differences . The participants were r and omly allocated ( 1:1 ) to intervention or control . CONCLUSIONS : In elderly volunteers 2 weeks of treatment with darifenacin had no effect on cognitive function compared with baseline and it was not significantly different from placebo . Existing overactive bladder treatments may cause adverse events , such as cognitive impairment , due to antagonism of the M1 receptor in the central nervous system . MATERIAL S AND METHODS Each treatment period was separated by 7 days of washout . Darifenacin was well tolerated . Eighty per cent of the continuous users were classified as having mild cognitive impairment compared with 35 % of non-users , and anticholinergic drug use was a strong predictor of mild cognitive impairment ( odds ratio 5.12 , P = 0.001 ) . Design Longitudinal cohort study . Setting 63 r and omly selected general practice s in the Montpellier region of southern France . Participants 372 people aged > 60 years without dementia at recruitment . Compared with non-users , they had poorer performance on reaction time , attention , delayed non-verbal memory , narrative recall , visuospatial construction , and language tasks but not on tasks of reasoning , immediate and delayed recall of wordlists , and implicit memory . On average , a 1-unit increase in the total anticholinergic burden per 3 months was associated with a 0.32-point ( 95 % confidence interval (CI)= 0.05 - 0.58 ) and 0.10-point ( 95 % CI=0.04 - 0.17 ) decrease in the HVRT and IADLs , respectively , independent of other potential risk factors for cognitive impairment , including age , education , cognitive and physical function , comorbidities , and severity of hypertension . CONCLUSION Cumulative anticholinergic exposure across multiple medications over 1 year may negatively affect verbal memory and executive function in older men . DESIGN Prospect i ve cohort study . SETTING A Department of Veterans Affairs primary care clinic . PARTICIPANTS Five hundred forty-four community-dwelling men aged 65 and older with diagnosed hypertension . Delirium was assessed using the DSM\\u2010IV\\u2010TR criteria and the Delirium Rating Scale , in a sample of consecutive patients with an acute ( \\u22644 days ) cerebral infa rct or intracerebral haemorrhage ( ICH ) . The variables associated with delirium on bivariate analysis were entered in a stepwise logistic regression analysis . We hypothesized that the intake of medications with ACH activity is associated with delirium in acute stroke patients . We performed a gender and age matched case \\u2013 control study . Medication with ACH activity should be avoided in acute stroke patients OBJECTIVE The aim of this study was to assess the prevalence of anticholinergic symptoms , corresponding symptom burden , and anticholinergic-related ADEs in a sample of community-dwelling elderly veterans . Anticholinergic-related ADEs were rare ( 0.8 % ) . ( both , P < 0.01 ) . The mean number of anticholinergic symptoms was significantly higher in the group using anticholinergic drugs ( 3.1 vs 2.5 ; P < 0.01 ) . METHODS This prospect i ve cohort study was conducted at the primary care clinics at the Veterans Affairs Medical Center ( VAMC ) , Iowa City , Iowa . Methods This cross-sectional , prospect i ve study ( 3-month telephone follow-up ) was conducted in 66 Italian internal medicine and geriatric wards participating in the Registry of Polytherapies SIMI ( Societ\\u00e0 Italiana di Medicina Interna ) ( REPOSI ) study during 2010 . Objective The aim of the study was to evaluate the association between anticholinergic burden and both cognitive and functional status , according to the hypothesis that the cumulative anticholinergic burden , as measured by the Anticholinergic Cognitive Burden ( ACB ) Scale and Anticholinergic Risk Scale ( ARS ) , increases the risk of cognitive decline and impairs activities of daily living . Results The sample included 1,380 in patients aged 65 years or older . No correlation was found with length of hospital stay . Results : The unadjusted follow-up mortality was 20.7 % and 9.5 % among the users and non-users of DAPs , respectively ( p = 0.010 ) . Abstract Background : Many potentially inappropriate drugs prescribed to older people have anticholinergic properties as adverse effects and are therefore potentially harmful . Conclusions : The use of DAPs in older patients with stable CVD was associated with an increased number of hospital days but not with mortality [ p < 0.001 ] . These effects typically include constipation , dry mouth , blurred vision , dizziness and slowing of urination . RESULTS There was a significant main effect of anticholinergic group averaged across time for the Symbol Digits Modalities Test with poorer performance among anticholinergic medication users . METHODS A r and omly selected community-based sample of 2058 persons aged 60 - 64 at baseline was interviewed twice over four years . Main effects for the other cognitive tests and mild cognitive impairment were non-significant . Cognition was assessed with the California Verbal Learning Test I ( one trial ) , Digits Backwards , the Symbol Digit Modalities Test , the Mini-Mental State Exam and simple and choice reaction time . CONCLUSIONS This study suggests that exposure to anticholinergic medication is associated with lower level of complex attention in the young-old , but not with greater cognitive decline over time . Objectives : To assess the cognitive effects of single doses of solifenacin 10 mg compared with placebo ( primary objective ) and oxybutynin immediate release ( IR ) 10 mg ( secondary objective ) in elderly subjects . Cognitive Drug Research computerised assessment system . Aspects of attention , information processing , working memory , episodic memory and self-rated mood and alertness were tested using the vali date d Methods : Single-centre , r and omised , double-blind , placebo-controlled study in 12 healthy elderly volunteers , with three crossover periods separated by two 14-day washout periods . Post-hoc ANCOVA showed no statistically significant cognitive deterioration with solifenacin versus placebo , when measured at a time point closest to the probable Cmax of solifenacin . MEASUREMENTS Medication history was obtained on each participant . After adjusting for age , sex , history of falling , cognitive impairment , depression , use of a walking aid , comorbidities , polypharmacy , and incontinence , incident rate ratios of 1.61 ( 95 % CI=1.17 - 2.23 ) for low DBI and 1.90 ( 95 % CI=1.30 - 2.78 ) for high DBI were obtained . Mean age was 85.7 \\u00b1 6.4 , and mean DBI was 0.60 \\u00b1 0.66 . OBJECTIVES To evaluate the association between the Drug Burden Index ( DBI ) , a measure of a person 's total exposure to anticholinergic and sedative medications that includes principles of dose-response and maximal effect and is associated with impaired physical function in community-dwelling older people , and falls in residents of residential aged care facilities ( RACFs ) . SETTING RACFs in Sydney , Australia . No differences in the effects of the drugs on any of the cognitive tests were noted . Objective The effects of risperidone and olanzapine on cognitive functioning in patients with schizophrenia were compared in a r and omized , double-blind trial . Cognitive function was assessed with a focused cognitive assessment battery ; in addition , extrapyramidal symptoms were assessed using the extrapyramidal symptom rating scale ( ESRS ) , and the positive and negative syndrome scale ( PANSS ) was rated for all patients . These results are not due in general to changes in clinical symptoms or movement disorders , suggesting a direct effect of atypical antipsychotic medications on cognitive deficits in schizophrenia Correcting for the effects of anticholinergic treatment did not alter the magnitude of cognitive effects . OBJECTIVES To describe the association between anticholinergic medications and incident delirium in hospitalized older adults with cognitive impairment and to test the hypothesis that anticholinergic medications would increase the risk of incident delirium . MEASUREMENTS Cognitive function at the time of admission was assessed using the Short Portable Mental Status Question naire ( SPMSQ ) . CONCLUSION DESIGN Observational cohort study . SETTING Urban public hospital in Indianapolis , Indiana .\",\n          \"Autonomic cardiovascular dysfunction accompanies AMS . Conversely , LF(SBP ) increased significantly during short-term hypoxia only in subjects with AMS , who also had higher resting BP ( P < 0.05 ) than those without AMS . Forty-one mountaineers were studied at 4,559-m altitude . Subjects with AMS were older than those without AMS ( P < 0.01 ) . Seventeen subjects ( 41 % ) had AMS . In contrast , 14 of the 18 adults who suffered from AMS on the first occasion also presented with this problem during the second exposure , and no new case developed in those who had not experienced AMS on the first occasion . OBJECTIVE : CONCLUSIONS : In adults , a history of AMS is highly predictable of the disease on subsequent exposure , whereas in children it has no predictive value . Although a history of previous acute mountain sickness ( AMS ) is commonly used for providing advice and recommending its prophylaxis during subsequent exposure , the intraindividual reproducibility of AMS during repeated high-altitude exposure has never been examined in a prospect i ve controlled study . RESULTS : Pulse oximetry measurements and venous blood sample s were obtained . Similarly , comparison of sex , age , rate of ascent , pulse oximetry values , or history of altitude illness did not reveal significant differences between the AMS and non-AMS groups . These results were nonsignificant . The purpose of this study was to determine if elevated plasma VEGF correlates with increased symptoms of AMS at high altitude . Of the 51 climbers participating in the study , 14 subjects ( 27.5 % ) had symptoms of AMS and 37 subjects ( 72.5 % ) were free of symptoms of AMS . Forty-seven climbers participated in a double-blind , r and omized trial comparing acetazolamide 250 mg , dexamethasone 4 mg , and placebo every eight hours as prophylaxis for acute mountain sickness during rapid , active ascent of Mount Rainier ( elevation 4,392 m ) . Forty-two subjects ( 89.4 percent ) achieved the summit in an average of 34.5 hours after leaving sea level . At the summit or high point attained above base camp , the group taking dexamethasone reported less headache , tiredness , dizziness , nausea , clumsiness , and a greater sense of feeling refreshed ( p less than or equal to 0.05 ) . This study demonstrates that prophylaxis with dexamethasone can reduce the symptoms associated with acute mountain sickness during active ascent and that acetazolamide can cause side effects that may limit its effectiveness as prophylaxis against the disease In addition , they reported fewer problems of runny nose and feeling cold , symptoms unrelated to acute mountain sickness .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/2021 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cd642a5727c4146ac475667867c607e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys of tokenized dataset: ['review_id', 'pmid', 'title', 'abstract', 'target_text', 'background', 'input_text', 'input_ids', 'attention_mask', 'labels']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained model!\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "final_save_dir = \"longt5-qlora-4-epochs-final\"\n",
        "config = PeftConfig.from_pretrained(os.path.join(output_dir, final_save_dir))\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(output_dir, final_save_dir))\n",
        "tokenizer = AutoTokenizer.from_pretrained(os.path.join(output_dir, final_save_dir))\n",
        "\n",
        "model = PeftModel.from_pretrained(model, os.path.join(output_dir, final_save_dir)).to(device)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "p936nbGay0n_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "c54dcf63195d4dd48eba8f05fbe7510d",
            "f8abff75537249ef8ec7a8204754a4be",
            "5a00dfea9e2546b18eb5accb262f441f",
            "35b026e73ceb4283aca19d65d03d1847",
            "be65e2f5d4924b0eaccdab4c234f07eb",
            "1d0a1221e1234e29b65e251849ffc73b",
            "41919fd5f31743a9957e5633af00100f",
            "28c24f0f162144fea396a1d8e5575aaf",
            "da1e32028fdc427eaa669c6553931864",
            "bfefc6d842bc4501b12eada6c4241251",
            "4d92e56a732e476ebb1dc487a133d2df",
            "7ca4afc56c0841d4b0437a2d3763f23c",
            "d8f625e0010c4d4886c0c140cd23b2d3",
            "360a78fb5fdb4a0194f10c190b6ede65",
            "fb4d7ce9a5664ab2b176068b7a405d7a",
            "d5d23724302649f7a4650c85633f0b05",
            "c24a68d4d99545cb9fde7f5630ef73b6",
            "a438698514af4d15aee04334cb3aaa4b",
            "2cacb019b8664ff9bf209b4f83029f7d",
            "049e01a80cbb45b392c38e5be899d0db",
            "8362f0352eec4c6f9a56e4c6849fa656",
            "59275ff7ebbd44fbbafdbc91fdcea170"
          ]
        },
        "outputId": "3b3fd26a-338c-47df-a6b2-c3a640d8cb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c54dcf63195d4dd48eba8f05fbe7510d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ca4afc56c0841d4b0437a2d3763f23c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\").shape)"
      ],
      "metadata": {
        "id": "A3xjyb2zzC3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5c6f1d-26f9-4dc5-bf4d-617e8e21f58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0457,  0.0409,  0.0206,  ..., -0.0350,  0.0114,  0.0443],\n",
            "        [ 0.0236,  0.0163,  0.0056,  ...,  0.0046, -0.0030,  0.0189],\n",
            "        [-0.0067,  0.0025,  0.0173,  ..., -0.0040, -0.0092,  0.0102],\n",
            "        ...,\n",
            "        [-0.0105, -0.0284,  0.0229,  ..., -0.0418, -0.0465, -0.0166],\n",
            "        [-0.0003,  0.0033,  0.0088,  ...,  0.0819,  0.0046, -0.0212],\n",
            "        [ 0.0027,  0.0263, -0.0178,  ..., -0.0667, -0.0055,  0.0349]],\n",
            "       device='cuda:0')\n",
            "torch.Size([768, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 100\n",
        "inputs = tokenized_test_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128,\n",
        "    num_beams=4,\n",
        ")\n",
        "print(\"BACKGROUND\")\n",
        "pprint(tokenized_test_dataset[\"background\"][id_to_choose])\n",
        "print(\"GENERATED\")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "print(\"TARGET\")\n",
        "pprint(tokenizer.decode(tokenized_test_dataset[id_to_choose]['labels'][tokenized_test_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "cp8mZmEbzGV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68b7e25-aaec-4ba1-9c64-72574ed82489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:932: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BACKGROUND\n",
            "('Home-based resistance exercise is commonly used for individuals who might '\n",
            " 'not have access or the ability to use traditional resistance exercise .\\n'\n",
            " 'However , the extent to which home-based resistance exercise can improve '\n",
            " 'both strength and functional ability has not been investigated in healthy '\n",
            " 'older individuals using a systematic analysis .')\n",
            "GENERATED\n",
            "('Conclusions This systematic review provides evidence that home-based '\n",
            " 'strength training is associated with improved functional independence in '\n",
            " 'older adults.')\n",
            "TARGET\n",
            "('Overall, home-based resistance exercise can improve both strength and '\n",
            " 'functional ability, but the improvements are generally small. The intensity '\n",
            " 'of the exercises might not progress sufficiently enough to produce large '\n",
            " 'improvements in strength as a result of less supervision or a lack of '\n",
            " 'motivation to increase the intensity further')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Test Samples"
      ],
      "metadata": {
        "id": "hRdIN1jwGxWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer batch decode\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "NsxqkhUvzU69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f65c5c5-3142-4b00-94f4-cf0aba5776b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Conclusions This systematic review provides evidence that home-based strength training is associated with improved functional independence in older adults.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "cfT8AHK4zbH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now generate for all test examples, save to disk for evaluation elsewhere\n",
        "\n",
        "def generate_and_save(dataset, save_path, batch_size=8):\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    # generate\n",
        "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
        "        subset_dataset = dataset.select(range(i, (i + batch_size) if (i + batch_size) < len(dataset) else len(dataset)))\n",
        "        inputs = subset_dataset[:]\n",
        "\n",
        "        # check if generated summaries already exist. check individual files\n",
        "        rows_to_keep = []\n",
        "        for idx, review_id in enumerate(subset_dataset['review_id']):\n",
        "            if os.path.exists(os.path.join(save_path, f\"{review_id}.txt\")):\n",
        "                continue\n",
        "            else:\n",
        "                rows_to_keep.append(idx)\n",
        "\n",
        "        if len(rows_to_keep) == 0:\n",
        "            continue\n",
        "\n",
        "        inputs = subset_dataset.select(rows_to_keep)[:]\n",
        "\n",
        "        output = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"].to(device),\n",
        "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "            max_new_tokens=128,\n",
        "            num_beams=4,\n",
        "        )\n",
        "        generated_summaries = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "\n",
        "        # save individually\n",
        "        for review_id, summary in zip(subset_dataset['review_id'], generated_summaries):\n",
        "            with open(os.path.join(save_path, f\"{review_id}.txt\"), 'w') as f:\n",
        "                f.write(summary)\n",
        "\n",
        "    # save aggregated into csv\n",
        "    # open all files\n",
        "    all_generated_summaries = {}\n",
        "    for fpath in os.listdir(save_path):\n",
        "        if fpath.endswith('.txt'):\n",
        "            all_generated_summaries[fpath.split('.')[0]] = open(os.path.join(save_path, fpath), 'r').read()\n",
        "\n",
        "    all_generated_summaries_sorted = [all_generated_summaries[rid] for rid in dataset['review_id']]\n",
        "    df = pd.DataFrame({\n",
        "        'review_id': dataset['review_id'],\n",
        "        'summary': all_generated_summaries_sorted\n",
        "    })\n",
        "    df.to_csv(os.path.join(save_path, 'generated_summaries.csv'), index=False)\n",
        "    print(f\"Saved generated summaries to {os.path.join(save_path, 'generated_summaries.csv')}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df_generated = generate_and_save(\n",
        "    tokenized_test_dataset,\n",
        "    f\"{output_dir}/generated_summaries\",\n",
        "    batch_size=2,\n",
        ")"
      ],
      "metadata": {
        "id": "ZFXIl3_7zgBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6e32a1-a649-407b-ce49-72f764016375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1011/1011 [55:35<00:00,  3.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved generated summaries to /drive/MyDrive/lora3/training_history/generated_summaries/generated_summaries.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_generated.shape)\n",
        "df_generated"
      ],
      "metadata": {
        "id": "9QQZDNy7zkNW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "d7f540b4-8518-4508-f503-eddf6f3323e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2021, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     review_id                                            summary\n",
              "0     28514886  Conclusions The results of this systematic rev...\n",
              "1     18842808  Conclusions The results of this meta- analysis...\n",
              "2     24297836  Conclusions : This meta- analysis suggests tha...\n",
              "3     32367221  There was no statistically significant differe...\n",
              "4     25038833  There was no evidence of an association betwee...\n",
              "...        ...                                                ...\n",
              "2016  19776504  In conclusion, the results of this systematic ...\n",
              "2017  27505198  Conclusions : There is insufficient evidence t...\n",
              "2018  25251296  The results of this meta- analysis suggest tha...\n",
              "2019  23235652  There was no evidence of a significant effect ...\n",
              "2020  30058911  Conclusions : This meta- analysis suggests tha...\n",
              "\n",
              "[2021 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9777e599-446b-4c42-82a7-1d849f7a999e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28514886</td>\n",
              "      <td>Conclusions The results of this systematic rev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18842808</td>\n",
              "      <td>Conclusions The results of this meta- analysis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24297836</td>\n",
              "      <td>Conclusions : This meta- analysis suggests tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32367221</td>\n",
              "      <td>There was no statistically significant differe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25038833</td>\n",
              "      <td>There was no evidence of an association betwee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016</th>\n",
              "      <td>19776504</td>\n",
              "      <td>In conclusion, the results of this systematic ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017</th>\n",
              "      <td>27505198</td>\n",
              "      <td>Conclusions : There is insufficient evidence t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018</th>\n",
              "      <td>25251296</td>\n",
              "      <td>The results of this meta- analysis suggest tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019</th>\n",
              "      <td>23235652</td>\n",
              "      <td>There was no evidence of a significant effect ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>30058911</td>\n",
              "      <td>Conclusions : This meta- analysis suggests tha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2021 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9777e599-446b-4c42-82a7-1d849f7a999e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9777e599-446b-4c42-82a7-1d849f7a999e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9777e599-446b-4c42-82a7-1d849f7a999e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-42e36156-78d2-4ea0-8200-e2d5cebf628d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42e36156-78d2-4ea0-8200-e2d5cebf628d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-42e36156-78d2-4ea0-8200-e2d5cebf628d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_fe8d6f5b-829b-433b-98d1-31caa65d22a9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_generated')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_fe8d6f5b-829b-433b-98d1-31caa65d22a9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_generated');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_generated",
              "summary": "{\n  \"name\": \"df_generated\",\n  \"rows\": 2021,\n  \"fields\": [\n    {\n      \"column\": \"review_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2021,\n        \"samples\": [\n          \"24100440\",\n          \"31528342\",\n          \"28718394\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1971,\n        \"samples\": [\n          \"Conclusions : There is no evidence to support or refute the association between physical activity and obesity in children.\",\n          \"Conclusions : The results of this review suggest that the use of high LA intake may be associated with an increased risk of CVD in healthy adults.\",\n          \"Conclusions : The results of this systematic review suggest that lifestyle interventions may be effective in reducing the incidence of Type 2 diabetes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "JWi7pYyWzn6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rouge Testing"
      ],
      "metadata": {
        "id": "MnPXQxqdG10J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split='validation')\n",
        "val_df = pd.DataFrame(validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "369ef19db1bd4945822b0bbd38cfb7f8",
            "8c60a26c03b34238a9c7605f7ff02110",
            "3373825265914df1afbea22c1e754dbc",
            "b8d2bed75aaa4fc0a56a25f9c7d353ad",
            "6e02a617f40b4d1bbcc4f8932c48aa86",
            "a6cf248920f54f7a95291f6dae68f2e5",
            "233e4ebe9fa54e5789cea99738e14415",
            "f98692d7f9e24462b0060bc194c7b884",
            "bfc7d85bbcd14adfb5fc1eaec076dff8",
            "81d59285901b4109b15ff126d8e17367",
            "2d645065d82d4d81ac13895a32518499",
            "fce0ee2e40e846b18175bb5c1fcd7fb9",
            "add4683fd03a4bdfad7963f75e2e922e",
            "a27b6081b0934dd49d1cf45427cf540d",
            "674aaa178ab840bd99c7dc65054a8795",
            "fe622282673e4bf6a1fbd38574ada83e",
            "f09fc1ebb6e64fc0abd0923175ac71e3",
            "6fc4465b344b42e892eaaf9f87cac76d",
            "48614291d2224aceb738df56496a6134",
            "2ba9e8021d744cd99ced2bb91aa4ae55",
            "616847af91dd41d7b7512ef5f2b64309",
            "444076d2e6a34048b6566a672c53b35e",
            "17622eb4b5c5401eba08644ee30eb1ef",
            "665df60c49b7457f934180e6903f0a69",
            "e89b3d3ea574470193e2101ef958d059",
            "fc4cd4d23622499abbcd30189044eb04",
            "4327990ac9c842a1a2a0a6d2b64c608d",
            "924c6ae76f1a434fa7a1b711698856b8",
            "9cc77d2db4d147a2b744d238bc588028",
            "0858dfd519254007a1a013c20ada670c",
            "9c42af913fdb41e3ba6fe7cfeee106f7",
            "862927a54827468fa423b10814be047b",
            "d5a67f24ba75401a8f9567d39ac248c7",
            "66f2b49f3a454b0a9b0c91d5c2eedff7",
            "4813d4a7f75c4594b7839e51208e0a54",
            "3a3ac1325458414b9767b6b1048a0323",
            "4115d45877254543ad331bd29e064fc2",
            "40ad83c157264f128a30e9111979ba77",
            "73f1884768fe416ab6705aa0874cba9f",
            "6ddeb287e9c54406882ae0511fb655fa",
            "55ae228d569848c8b80f4bf9e9550062",
            "09cefc5ab6f0410d8ffcb18323aeca0f",
            "1e5f32107e664210a8b407b542189341",
            "599fc65c032343b09b246c1dcf440110",
            "76caa49180534417bda0f230f7ee693e",
            "54cff3954c2f4f3a8420a48e63189c1a",
            "10ae6e9d200548de900b80c948d73eed",
            "da7cdf365a4844129a5ef3d9752b1be1",
            "cbf90dfe961b408d9922f1caba2184a5",
            "817d5de9a47245d296ac1cc6b228c2ae",
            "395355072bd94afaa9c35969e7269d66",
            "390cc68c80e441d89aa959303f946ed6",
            "a2c56cf289db4b3d99e94d15e13af5fd",
            "46f4b5ee319e4f049198e8d694adf15f",
            "7bd30cf43aa74814bbffe05921b7e2b1",
            "ea3510fc5d0546c39f34cec6b6d4abfb",
            "5bffed69e2014102a0917dacd3ce8f85",
            "bbadd3e25ead4466931dd5e4bb778346",
            "aaa4ef4e134449be9029cdee17bd034e",
            "f07486d4226b46ed9b4f6c893a9d89d5",
            "18498703326a4fd68d421ef4e3d5f76c",
            "780237a621d04301b20ca1ba7d96e351",
            "877b7ac073a9425896b05b9713c86ce7",
            "a43d63e319be407fa1acde1fa9fbf250",
            "9862ef3fb0f04166a90c6274e42e6278",
            "72a15c9ab3a34320bf10fa041f781664",
            "e814e66070714b5fba3bdaffd7e798e5",
            "f9d001cd16ae433286546124913439f3",
            "a327fb33737d445b8b68ecd5c36fd6ad",
            "34edcabed7974517a7176983bf5c82b0",
            "60a9b8fa2a1c4ec6b22b376dcd778aa6",
            "40004c7051214066a28759e88723a833",
            "6f78453bbbe7493ca8a5a4ee8ec3e2dc",
            "7253d956bfc9422d9eb75c1b9651a2ac",
            "bb00e9944d3e465da92c3ec14d9c6428",
            "ebdf7978f15f43199a20378bbe15bded",
            "61e9e21972774207bf1e511b87837b94"
          ]
        },
        "id": "aSvBQvUe7kyo",
        "outputId": "3770ff52-1b83-427f-d86d-428c8e445e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/260M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "369ef19db1bd4945822b0bbd38cfb7f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/48.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fce0ee2e40e846b18175bb5c1fcd7fb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/39.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17622eb4b5c5401eba08644ee30eb1ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/46.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66f2b49f3a454b0a9b0c91d5c2eedff7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/14188 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76caa49180534417bda0f230f7ee693e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1667 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea3510fc5d0546c39f34cec6b6d4abfb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/2021 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e814e66070714b5fba3bdaffd7e798e5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "57Fe_V5M8Ake"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "predictions = df_generated['summary']\n",
        "references = val_df['target']\n",
        "rouge_results = rouge.compute(predictions=predictions,\n",
        "                        references=references)\n",
        "print(rouge_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljLVp_uz7xpU",
        "outputId": "35b87a7a-e8c0-4c54-f5b4-0632bcb657c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.18267268616252466, 'rouge2': 0.02901514483366999, 'rougeL': 0.13566238304056688, 'rougeLsum': 0.15032992923345617}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEURT Evaluation"
      ],
      "metadata": {
        "id": "YNLD_o3AG5F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/google-research/bleurt.git\n",
        "from datasets import load_metric\n",
        "\n",
        "# Load BLEURT from datasets\n",
        "bleurt = load_metric('bleurt')"
      ],
      "metadata": {
        "id": "VX3K307tCFS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = df_generated['summary']\n",
        "references = val_df['target']\n",
        "# Compute BLEURT scores\n",
        "bleurt_results = bleurt.compute(predictions=predictions, references=references)\n",
        "\n",
        "# Print BLEURT scores\n",
        "print('bleurt_results:', bleurt_results)\n",
        "print('Avg BLEURT Score:', str(sum(bleurt_results['scores'])/len(bleurt_results['scores'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4468EssElRg",
        "outputId": "4ddb73a7-061b-4aa2-f8d5-45efc55ea352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bleurt_results: {'scores': [-0.43906593322753906, -0.7201102375984192, -0.6414153575897217, -1.0752925872802734, -0.04353736713528633, -0.7584176063537598, -0.5362364649772644, -0.4554423987865448, -1.1173832416534424, -0.21510204672813416, -0.6734787225723267, -0.756641149520874, -0.8470857739448547, -0.6999427676200867, -0.661134660243988, -0.9691223502159119, -0.41213589906692505, -0.7942270040512085, -0.14714835584163666, -0.7971200942993164, -0.6537171006202698, -0.9342362284660339, -0.9086545705795288, -0.8355856537818909, -0.2189977616071701, -0.5718160271644592, -0.1945147067308426, -1.6465190649032593, -1.0098047256469727, -0.9226223826408386, -0.8383930325508118, -0.7827581763267517, -1.0066509246826172, -0.44402265548706055, -0.9188247323036194, -1.3416494131088257, -0.9707406163215637, -0.40680429339408875, -1.013472318649292, -0.8357240557670593, -0.5488554835319519, -0.12399326264858246, -0.8670968413352966, -0.1156061440706253, -0.5830026865005493, -0.8187044262886047, -0.6734748482704163, -0.7974902987480164, -0.10077856481075287, -1.033638834953308, -1.0387003421783447, -1.0185738801956177, -0.9666640162467957, -0.7492188215255737, -0.43829068541526794, -1.1435515880584717, -0.25232067704200745, -1.245039939880371, -0.8876569867134094, -1.228959560394287, -0.40718647837638855, -0.9074342250823975, -1.2824207544326782, -0.5559686422348022, -0.7238819003105164, -0.9646406173706055, -0.7819651961326599, -0.4793142080307007, -0.9072310924530029, -1.1546159982681274, -0.4823460578918457, -0.729854941368103, -0.6755032539367676, -0.6666113138198853, -0.8156430125236511, -0.7293199896812439, -0.525597870349884, -1.0559748411178589, -0.9213669896125793, -1.212320327758789, -0.7641733288764954, -0.8843129873275757, -0.9447914958000183, 0.002970602363348007, -0.5610293745994568, -1.3300563097000122, -0.44573599100112915, -1.3574657440185547, -0.4374874234199524, -0.9178504347801208, -1.0467329025268555, -0.953617513179779, -0.9473828077316284, -0.8714444041252136, -0.11638402938842773, -0.8062565326690674, -1.0615286827087402, -0.8951026797294617, -0.6986849904060364, -0.728766679763794, -0.6958726048469543, -0.5994958877563477, -0.7676805257797241, -1.1261212825775146, -0.9713034629821777, -0.8429004549980164, -0.5256960391998291, -0.9834915399551392, -1.2197275161743164, -0.7030191421508789, -1.246586561203003, -0.6283464431762695, -0.6050370931625366, -0.9364129304885864, -0.9133293628692627, -0.6652913689613342, -1.4744707345962524, -0.8174052834510803, -0.6301177144050598, -0.9994208216667175, -0.34715574979782104, -0.4387286603450775, -0.44873350858688354, -0.7503207325935364, -0.2612946331501007, -0.2188083827495575, -1.0984786748886108, -0.5564094185829163, -0.8661769032478333, -0.15874828398227692, -0.45173460245132446, -0.6662576794624329, -1.2297812700271606, -0.5269705057144165, -0.8936856985092163, -0.47528302669525146, -1.1926980018615723, -1.1269739866256714, -0.48481640219688416, -0.9832223057746887, -1.4282660484313965, -0.495802104473114, -0.6045008301734924, -1.1340084075927734, -0.3466501235961914, -1.2382972240447998, -0.8731258511543274, -0.6146116852760315, -0.6833559274673462, -1.1094403266906738, -0.49346017837524414, -0.8395757079124451, -1.085040807723999, -0.08202248811721802, -0.32033294439315796, -0.9191353917121887, -0.6911524534225464, -0.5779001712799072, -0.18678422272205353, -0.489021897315979, -0.8444616794586182, -0.7631688117980957, -0.604759156703949, -0.6763255000114441, -0.2848495841026306, -1.1277798414230347, -0.8898627161979675, -1.1269359588623047, -0.6364298462867737, -0.82401043176651, -0.3617096245288849, -0.6519631743431091, -1.2170557975769043, -1.0372627973556519, -0.8776862025260925, -0.8679063320159912, -1.040661334991455, -0.010428253561258316, -0.9756187796592712, -0.7050414681434631, -0.14388485252857208, -0.7040053606033325, -0.7156990170478821, -0.9393211603164673, -0.9946063756942749, -0.9205307364463806, -1.0429209470748901, -0.8447940349578857, -0.6762656569480896, -1.1494756937026978, -0.7745378613471985, -1.0391976833343506, -0.7150666117668152, -0.8682359457015991, -0.6035586595535278, -1.1414319276809692, -0.7464569211006165, -0.8377544283866882, -0.43137556314468384, -0.7890323996543884, -1.4666099548339844, -1.0001659393310547, -0.7293990254402161, -1.117832064628601, -0.18413721024990082, -0.6538245677947998, -1.220069408416748, -0.769989013671875, -1.0937453508377075, -0.2725781202316284, -0.898242712020874, -0.4010063409805298, -0.6309180855751038, -0.43091583251953125, -0.8733770251274109, -0.8254672884941101, -0.41351670026779175, -0.6082167029380798, -0.35563915967941284, -1.0313189029693604, -0.8367801904678345, -0.8664677739143372, -1.172649621963501, -0.8674841523170471, -0.8613340854644775, -1.0441792011260986, -0.6461995244026184, -0.2577977776527405, -0.9507986903190613, -1.0442209243774414, -1.024912714958191, -0.816942036151886, -0.8610785007476807, -0.9613374471664429, -0.9092329144477844, -0.9854553937911987, -0.5248187184333801, -0.6348936557769775, -0.4651866853237152, -0.6428123116493225, -0.4738517999649048, -0.705889105796814, -0.17472368478775024, -1.0713765621185303, -1.193568229675293, -0.7075967192649841, -1.056496500968933, -0.774766206741333, -0.9388626217842102, -1.104994773864746, -1.1096781492233276, -0.2329532504081726, -0.7739559412002563, -0.5813302397727966, -1.444211483001709, -0.9585395455360413, -0.26100391149520874, -0.18948578834533691, -0.7931742072105408, -0.32326847314834595, -0.4861661195755005, -1.0598869323730469, -1.3321852684020996, -0.8103411793708801, -0.299862802028656, -0.8001829385757446, -0.5108910799026489, -0.763020932674408, -0.956706166267395, -0.6540750861167908, -1.2573904991149902, -0.5376330018043518, -0.8739014863967896, -0.49075260758399963, -0.1405876874923706, -0.14395150542259216, -1.1553664207458496, -0.1115809977054596, -0.6687515377998352, -0.08192640542984009, -0.24998293817043304, -0.9032441973686218, -0.43330565094947815, -0.538640022277832, -1.2333788871765137, -1.0647294521331787, -1.0239547491073608, -1.0100950002670288, -1.3446927070617676, -0.3138744533061981, -0.7809669971466064, -1.0281563997268677, -1.3979741334915161, -0.5740916132926941, -0.0035045333206653595, -1.27401864528656, -0.6074848175048828, -0.3955165147781372, -0.7665612697601318, -0.766135573387146, -0.5218406319618225, -0.4633863568305969, -0.7465113997459412, -0.5990314483642578, -1.295287847518921, -0.6756213307380676, -0.9779824018478394, -1.1195093393325806, -1.1180955171585083, -1.0298261642456055, -0.8744391798973083, -0.8623118996620178, -0.5036177635192871, -0.8781033158302307, -1.1100258827209473, -0.8757773041725159, -0.6456252336502075, -0.3692280054092407, -0.03056793287396431, -1.1542689800262451, -0.8428440690040588, -0.6678544878959656, -0.15789347887039185, -1.0264889001846313, -0.8203603625297546, -1.227698802947998, -0.8231303095817566, -0.06811562180519104, -1.1167126893997192, -0.7921193242073059, -0.7059743404388428, 0.048691656440496445, -0.5501348376274109, -0.9976599216461182, -1.2355929613113403, -0.26151901483535767, -0.9501305222511292, -0.46123796701431274, -0.6203694343566895, -0.8456097841262817, -0.5288001894950867, -1.0000183582305908, -0.6656913757324219, -0.7748734951019287, -0.5023621320724487, -0.7182689309120178, -0.7632203698158264, -1.2927664518356323, -1.0787020921707153, -0.5089399218559265, -0.5896297097206116, -0.3975202441215515, -1.511405110359192, -0.7770146131515503, -0.6891223788261414, -0.4745399057865143, -1.0541189908981323, -1.0300240516662598, -0.7565966248512268, -0.3100495934486389, -0.9526653289794922, -1.079268455505371, -0.5046771168708801, -0.6454432010650635, -0.8491209149360657, -1.006150722503662, -1.0662319660186768, -0.6711254119873047, -0.7507361173629761, -0.9153419733047485, -0.4660889804363251, -0.8115717768669128, -0.40510714054107666, -0.29286855459213257, -0.4953267574310303, -0.4076175391674042, -1.2106997966766357, -0.48110947012901306, -0.6969234943389893, -0.8026749491691589, -0.8302851319313049, -0.49032074213027954, -0.3340819478034973, -1.0244754552841187, -1.0504041910171509, -0.9844862818717957, -0.8516381978988647, -0.5443909168243408, -0.6655147671699524, -0.5283265709877014, -1.045060157775879, -0.2101779580116272, -0.0009925998747348785, -1.199047565460205, -0.6967886686325073, -0.8505510091781616, -0.05096087232232094, -0.828706681728363, -0.6553835868835449, 0.13764427602291107, -1.2323033809661865, -1.0606815814971924, -0.6683520674705505, -0.23047713935375214, -0.7904835939407349, -0.7237695455551147, -0.9049674272537231, -1.0254535675048828, -0.8180504441261292, -1.2931889295578003, -0.7847958207130432, -0.49664437770843506, -0.8727105259895325, -0.2802478075027466, -0.9692147374153137, -0.9027634263038635, -0.7344984412193298, -0.08938674628734589, -0.7187960743904114, -1.095384120941162, -0.7014108896255493, -1.285727858543396, -1.0671298503875732, -0.9142811894416809, -1.0256260633468628, -1.1020126342773438, -0.9899275898933411, -0.5222213864326477, -0.7245379090309143, -0.4798930287361145, -0.47271648049354553, -0.35449540615081787, -0.5888885259628296, -1.1576778888702393, -0.4854966700077057, -0.7063964009284973, -0.6875943541526794, -0.3797062933444977, -0.9639806151390076, -0.2441893368959427, -0.6462177038192749, -0.40166664123535156, -0.6742830872535706, -0.4380592405796051, -1.3384522199630737, -0.7560257315635681, -1.097536563873291, -0.5632004141807556, -1.3000324964523315, -0.3760080933570862, -0.9561867117881775, -0.6765331625938416, -0.5792019367218018, -0.32694172859191895, -0.4742617607116699, -0.9571263194084167, -0.599373459815979, -1.1644867658615112, -1.1635745763778687, -0.9196240305900574, -0.16211354732513428, -0.752730667591095, -1.296205997467041, -0.3677818179130554, -0.5760665535926819, 0.0523492805659771, -0.8547370433807373, -0.5361263155937195, -1.0519418716430664, -0.9881910681724548, -0.8647508025169373, -0.6645622849464417, -0.9325196146965027, -0.9507388472557068, -0.3586532473564148, -0.7537949681282043, -0.558741569519043, -1.0460541248321533, -0.4013046324253082, -0.9176656603813171, -0.9494088888168335, -0.5156391859054565, -0.6504383683204651, -1.352042317390442, -0.7598265409469604, -0.966525673866272, -0.5179355144500732, -0.656425952911377, -0.31224900484085083, -0.9702154994010925, -0.4590449929237366, -1.151412010192871, -0.9184768795967102, -1.1787703037261963, -1.1339031457901, -0.7695725560188293, -0.3243749141693115, -1.0007203817367554, -0.585633397102356, -0.5961063504219055, -0.80205237865448, -0.33579522371292114, -1.041171908378601, -0.9262609481811523, -0.8529771566390991, -0.8486344218254089, -0.7271938323974609, -0.5724717378616333, -1.0983703136444092, -0.8853986859321594, -0.7279366850852966, -0.6662355065345764, -0.9908804893493652, -0.7258991599082947, -0.265516459941864, -0.8299835324287415, -0.6402038335800171, -0.5906038284301758, -0.9157288074493408, -1.4264053106307983, 0.3109091818332672, -0.7327623963356018, -1.2043040990829468, -0.4181743860244751, -0.6618705987930298, -0.47831082344055176, -0.9650042653083801, -0.5745384097099304, -0.7933390140533447, -1.0331592559814453, -0.6829594969749451, -0.6509414911270142, -0.5060720443725586, -0.5985907912254333, -1.0498220920562744, -0.5006794929504395, -1.088819980621338, -0.9808875918388367, -0.776648759841919, -1.1234166622161865, -0.801314115524292, -0.9648042321205139, -0.3765934705734253, -0.7213721871376038, 0.01569492742419243, -0.8260069489479065, -0.617245078086853, -1.4001884460449219, -1.0029425621032715, -1.0036838054656982, -1.3562579154968262, -0.3333255350589752, -0.8141764998435974, -0.6558706164360046, -1.1829129457473755, -0.38089489936828613, -0.6923540830612183, -0.6465885043144226, -0.17947866022586823, -0.9612970948219299, -0.8791687488555908, -0.7241299748420715, -0.7028235197067261, -0.8084642887115479, -0.6960474848747253, -0.8237287402153015, -0.9006104469299316, -0.20842598378658295, -0.2092614769935608, -0.7246267199516296, -0.8547967076301575, -0.8960174322128296, -0.16364753246307373, -0.6866095662117004, -0.4712451696395874, -0.8224759697914124, -0.686051607131958, -0.6725000739097595, -0.8884170651435852, -0.3237987756729126, -0.9043439030647278, -0.9635986685752869, -0.6504810452461243, -0.31111347675323486, -0.4995729327201843, -0.4441991448402405, -1.001074194908142, -0.6870318055152893, -1.531125545501709, -0.6003189086914062, -0.6049608588218689, -0.968250036239624, -0.7078584432601929, -0.5149122476577759, -0.5898119211196899, -1.4685198068618774, -0.3918173909187317, -1.0122979879379272, -0.6801990866661072, -0.4383772015571594, -1.0992963314056396, -0.5207069516181946, -0.32685965299606323, -0.38841742277145386, -0.6005440950393677, -0.21225246787071228, -0.6776303052902222, -0.5267853140830994, -1.2398136854171753, -0.8387579321861267, -0.7589442133903503, -0.5922554135322571, -0.10982921719551086, -0.8001055121421814, -0.9009619355201721, -0.7027139067649841, 0.10088033974170685, -0.7582196593284607, -1.2099344730377197, -1.4541257619857788, -0.8840183615684509, -0.43397510051727295, -0.7993684411048889, -0.828617513179779, -0.9186439514160156, -0.6005050539970398, -1.004838228225708, -0.7964257597923279, -1.296205997467041, -0.9203916192054749, -1.0533219575881958, -0.654331386089325, 0.03443783149123192, -0.7844160199165344, -0.7830557823181152, -0.8308872580528259, -0.9362472891807556, -0.3447335362434387, -0.0015940405428409576, -1.2513096332550049, -0.4610791802406311, -1.0000379085540771, -1.0365889072418213, -0.7478539943695068, -0.6244115233421326, -1.2178672552108765, -0.7838447093963623, -1.155360460281372, -0.2587384581565857, -1.0437748432159424, -0.5269832015037537, -1.2476963996887207, -0.6681593060493469, -0.6928122639656067, -0.2508377134799957, -0.09620560705661774, -0.7478671669960022, -1.282145380973816, -0.914192795753479, -0.388621985912323, -0.8886007070541382, -0.48130691051483154, -0.7125688791275024, -0.3502601981163025, -1.0099200010299683, -0.6110588908195496, -1.0235944986343384, -0.6291922330856323, -0.8215329647064209, -0.9737979769706726, -0.5384235978126526, -0.9692680835723877, -1.0717527866363525, -1.1378273963928223, -0.6657509803771973, -0.11634987592697144, -0.6884250044822693, -0.8040146827697754, -0.8492789268493652, -0.7325455546379089, -0.8096807599067688, -1.1382704973220825, -0.6541466116905212, -0.6158396005630493, -0.879248857498169, -0.38285568356513977, -0.21750973165035248, -0.7295176982879639, -0.3830975890159607, -0.767466127872467, -0.9916472434997559, -1.0232278108596802, -0.5637898445129395, -0.7436216473579407, -0.6222032308578491, -0.7095822691917419, -1.2401982545852661, -0.8387019038200378, -1.2814580202102661, -1.2341047525405884, -0.6149449944496155, -1.0274779796600342, -0.8384357690811157, 0.031239788979291916, -0.783474326133728, -0.6271147727966309, -0.9156259894371033, -0.8655427098274231, -0.497349351644516, -0.593449056148529, -0.6168666481971741, -0.6333782076835632, -0.6589609980583191, -1.1074327230453491, -0.5977550745010376, -0.7953599095344543, -0.5766634345054626, -1.1796338558197021, -1.1205393075942993, -0.37445080280303955, -1.275360107421875, -1.1893914937973022, -0.3853057622909546, -0.2580946683883667, -0.4844367206096649, -0.47276338934898376, -0.6937264204025269, -0.9108611941337585, -1.30441415309906, -0.39274322986602783, -0.9501861929893494, -0.7453610301017761, -1.035454511642456, -0.6901453137397766, -0.39136019349098206, -0.5214791297912598, -1.173359990119934, -0.8790109157562256, -0.1907738298177719, -0.5440207123756409, -0.4414377808570862, -0.6207877993583679, -0.03574303165078163, -0.769472062587738, -1.035934329032898, -0.5066673159599304, -0.5213639736175537, -0.9103692173957825, -0.4192086458206177, -0.9828742742538452, -0.6053918600082397, -0.3394380211830139, -0.6674083471298218, -1.2642532587051392, -0.5935312509536743, -0.35491180419921875, -1.4089314937591553, -0.9356780052185059, -1.077236294746399, -0.558346152305603, -1.2469013929367065, -0.47652459144592285, -0.24298346042633057, -1.1124236583709717, -0.6466546654701233, -0.6643370985984802, -0.18943460285663605, -0.3941633701324463, -0.6408690810203552, -1.0885647535324097, -0.9957643151283264, -1.1566412448883057, -0.7934160828590393, -0.5687606930732727, -0.8354415893554688, -0.7368797063827515, -0.8322692513465881, -0.5728375315666199, -1.2039201259613037, -1.058614730834961, 0.014588814228773117, -1.1356853246688843, -0.7025840282440186, 0.06745894253253937, -0.5375916957855225, -0.4841964840888977, -1.0764288902282715, -0.8824101090431213, -0.4159948527812958, -0.46859925985336304, -0.6666965484619141, -1.6801410913467407, -0.8420308232307434, -0.4358249306678772, -1.0474576950073242, -1.086116909980774, -0.8255195021629333, -0.18949317932128906, -1.3922065496444702, -0.5706427693367004, -1.4672192335128784, -0.6807801127433777, -0.4733572006225586, -0.798173725605011, -0.8886768817901611, -0.8290301561355591, -0.3329480290412903, -0.8111985921859741, -0.5000202059745789, -0.7682563066482544, -1.0717287063598633, -0.3910318613052368, -0.9830157160758972, -1.1810302734375, -0.9486250281333923, -0.541001558303833, -0.9302414059638977, -1.0382765531539917, -0.32514896988868713, 0.22552353143692017, -1.061006784439087, -0.691946804523468, -0.9072335958480835, -0.3887300491333008, -0.8199209570884705, -0.22547955811023712, -0.4875463843345642, -1.1893409490585327, -0.3192974328994751, -0.7492596507072449, -0.2970307469367981, -0.9466776847839355, -0.6719931960105896, -0.47252678871154785, -0.9166167378425598, -0.7138140797615051, -0.07556521892547607, -0.45392394065856934, -0.47235333919525146, -0.9745212197303772, -0.8808934092521667, -0.7866461277008057, -0.9414540529251099, -0.523106575012207, -0.7450982928276062, -0.495508074760437, -0.5449191927909851, -0.6909968256950378, -0.7741536498069763, -0.6087422370910645, -0.950330913066864, -0.6710366606712341, -0.8351591229438782, -1.2978702783584595, -0.8611652851104736, -0.5850821137428284, -0.3756927251815796, -1.2248319387435913, -0.8151264786720276, -0.9264417290687561, -0.9824330806732178, -0.3736652135848999, -0.27741557359695435, -1.3711457252502441, -1.3003195524215698, -1.0316959619522095, -1.1470166444778442, -0.5683645009994507, -0.7859708070755005, -0.7730249762535095, -1.2589938640594482, -0.784660816192627, -1.2752268314361572, -0.49458009004592896, -1.309175968170166, -0.2977004051208496, -0.6392180323600769, -0.8120283484458923, -0.5312780141830444, -0.8417642712593079, -0.8379966616630554, -0.9583359360694885, -0.5897429585456848, -0.9313806295394897, -0.17373120784759521, -0.842848539352417, -0.33936429023742676, -0.6503245830535889, -0.8535361886024475, -0.42929208278656006, -0.7477492690086365, -0.4673359990119934, -0.41878342628479004, 0.01275235041975975, -1.0332086086273193, -0.613153874874115, -0.5854780077934265, -0.9716658592224121, -0.7664980292320251, -1.0861119031906128, -0.5892030000686646, -1.2317898273468018, 0.25802093744277954, -1.102832555770874, -0.6846714615821838, -1.0018141269683838, -0.5327407121658325, -1.1861218214035034, -0.9912516474723816, -0.8138015270233154, -1.0085116624832153, -0.9729683995246887, -1.1339515447616577, -0.83671635389328, -0.5202087163925171, -0.6547837257385254, -0.6748178005218506, -1.0726957321166992, -0.5718150734901428, -0.8776103258132935, -0.3919932544231415, -0.8213977813720703, -0.23168590664863586, -0.8785485625267029, -0.25458040833473206, -0.7845752835273743, -0.5629786849021912, -0.07604031264781952, -0.5703241229057312, -0.11048264801502228, -0.47284644842147827, -0.9063804745674133, -0.20548942685127258, -0.5165924429893494, -0.1004885584115982, -0.645880937576294, -0.4677909314632416, -1.37589430809021, -0.7800530791282654, -1.0987741947174072, -0.5466060042381287, -0.8655427098274231, -0.06012088432908058, -0.844255268573761, -0.5979047417640686, -0.9015429019927979, -1.0942587852478027, -0.42418426275253296, -1.124965786933899, -0.46881961822509766, -0.8973655104637146, -1.229607105255127, -0.7879393100738525, -1.100135087966919, -0.5209171772003174, -0.82391357421875, -0.855506956577301, -1.2368409633636475, -0.8264527916908264, -0.8977014422416687, -0.43477386236190796, -0.7883078455924988, -0.513454794883728, -1.0819222927093506, -0.6413244009017944, -0.631706178188324, -0.67103111743927, -0.3552224338054657, -0.47815728187561035, -0.8410477042198181, -0.5498184561729431, -0.9226810336112976, -0.5121483206748962, 0.050713639706373215, -0.7423973083496094, -0.5335171818733215, -0.6206642985343933, -0.8122140765190125, -1.3378980159759521, -0.2599766254425049, -0.9953551292419434, -0.8936242461204529, -1.169519305229187, -1.1059211492538452, -0.8051131963729858, -1.1405060291290283, -0.8641155958175659, -0.3567831516265869, -0.9654332995414734, -0.5087416172027588, -1.3484830856323242, -0.5387365221977234, -1.0604766607284546, -0.32313281297683716, -0.6839916706085205, -1.060983657836914, -0.9679775834083557, -0.7135183215141296, -0.727327823638916, -1.351001262664795, -0.9802246689796448, -1.0191731452941895, -1.5375828742980957, 0.05188317224383354, -0.5140713453292847, -1.2905724048614502, -0.529111385345459, -0.47830894589424133, -0.4366421699523926, -0.6122621893882751, -0.6019417643547058, -0.8764063119888306, -0.8684033751487732, -0.20376735925674438, -0.531109631061554, 0.10263147950172424, -0.3289549648761749, -1.2278835773468018, -0.5011346340179443, -0.8045334815979004, -1.1885082721710205, -0.6114783883094788, -0.8518041968345642, -0.690321147441864, -0.9896815419197083, -0.9105211496353149, -0.6557015776634216, -1.1753677129745483, -0.9138736724853516, -0.7089232802391052, -1.35338294506073, -1.0685977935791016, -0.3502134084701538, -0.6636532545089722, -1.1268712282180786, -0.746580958366394, -0.5198578238487244, -0.3440501391887665, -1.1996535062789917, -0.4956710934638977, -1.025709867477417, -0.7204847931861877, -1.0074174404144287, -0.7108616232872009, -0.5339708924293518, -1.007987380027771, -1.0079175233840942, -0.346200168132782, -0.6342169046401978, -0.636238157749176, -0.8847483396530151, -0.3536009192466736, -0.883454442024231, -0.16431891918182373, -0.918316662311554, -0.7476032376289368, -0.9130964279174805, -0.5622264742851257, -0.640643298625946, -0.1479426920413971, -0.7753105163574219, -0.2600022852420807, -1.4968458414077759, -0.49463462829589844, -0.6881710886955261, -0.6512061357498169, -0.5735030174255371, -1.7602440118789673, -0.8726065754890442, -0.7639529705047607, -0.6275346279144287, -0.8940553069114685, -0.15555810928344727, -1.0455143451690674, -1.5523362159729004, -0.7631540298461914, -1.129404902458191, -0.334857314825058, -0.06533338129520416, -0.5879137516021729, -0.3891061842441559, -0.8043369650840759, -0.9053007960319519, -0.913759708404541, -0.7615051865577698, -0.6700133681297302, -1.2910069227218628, -0.5100517868995667, -0.5152740478515625, -0.5810031890869141, -0.8113850355148315, -0.7942131161689758, -0.6766833066940308, -0.17075669765472412, -0.7635820508003235, -0.7234377264976501, -0.5065039992332458, -1.2163587808609009, -0.8110182881355286, -0.9486146569252014, -1.1818928718566895, -0.39723536372184753, -1.0904563665390015, -0.3383161127567291, -0.8185494542121887, -0.5755739212036133, -0.5138577818870544, -0.1983504295349121, -0.6881622672080994, -0.5359554290771484, -0.8240094780921936, -0.5893751978874207, -0.7020342350006104, -0.8810029625892639, -0.66124427318573, -0.7511528134346008, -0.9062181115150452, -1.4711171388626099e-05, -0.5411966443061829, -1.170776605606079, -0.7293310165405273, -1.0400049686431885, -0.712158739566803, -0.7851880192756653, -1.1376430988311768, -0.5753828287124634, -0.6270452737808228, -1.0643396377563477, -0.5569812059402466, -0.5091416835784912, -0.8539864420890808, -0.56362384557724, -1.048278570175171, -0.0637752115726471, -1.029463291168213, -1.0399237871170044, -0.4089575409889221, -0.23953722417354584, -1.084380030632019, -0.6034712195396423, -0.9306984543800354, -1.0082536935806274, -0.9466798305511475, -0.47345125675201416, -0.2372724711894989, -0.17920947074890137, -0.9437135457992554, -0.31858837604522705, -1.0527029037475586, -0.5292519927024841, -1.1787010431289673, -0.11409831047058105, -1.0633578300476074, -1.1221245527267456, -0.6918840408325195, -0.5914617776870728, -0.43497729301452637, -1.0893316268920898, -0.5247686505317688, -0.8697550892829895, -1.171775221824646, -0.2348923534154892, -0.6785812973976135, -1.1427850723266602, -0.34052687883377075, -0.6984350681304932, -1.0415600538253784, -0.5871127247810364, -0.911472737789154, -0.40765058994293213, -0.7462825775146484, -0.650695264339447, -0.449136346578598, -0.42017391324043274, -1.1312901973724365, -0.748622477054596, -1.2994707822799683, -0.6768011450767517, -0.27122554183006287, -0.7179740071296692, -0.8773655295372009, -0.9561805129051208, -0.07501794397830963, -0.5239925980567932, -0.792090117931366, -0.578134298324585, -0.833142876625061, -1.19242262840271, -0.3488137125968933, -0.809986412525177, -1.31203293800354, -0.7159227728843689, -0.6866462230682373, -0.015824083238840103, -0.5103430151939392, -0.5216755270957947, -0.5501881837844849, -0.4961082637310028, -0.5437989234924316, -0.8943078517913818, -0.3576962947845459, -0.5646244287490845, -1.1866875886917114, -0.3971584439277649, -1.012771725654602, -0.879776120185852, -0.5157591104507446, -0.5609768033027649, -0.3800013065338135, -0.63264399766922, -0.215541273355484, -0.6513450145721436, -0.4882645905017853, -0.7226234078407288, -0.1441166251897812, -0.7981271147727966, -0.9837110638618469, -0.6503379940986633, -0.5313352346420288, -0.26329857110977173, -0.5302504897117615, -0.6635876297950745, -0.9315754771232605, -0.798299252986908, -0.6209456920623779, -0.7941142916679382, -0.717971682548523, -0.6907522678375244, -0.5347434878349304, -0.7042739391326904, -1.129380226135254, -0.20681430399417877, -1.3304169178009033, -0.6249147057533264, -0.6190040707588196, -0.5478127002716064, -0.5011300444602966, -1.0227285623550415, -0.7483091950416565, -0.25590550899505615, -0.5785414576530457, -0.9647874236106873, -0.22463159263134003, -0.7170757055282593, -0.49450451135635376, -0.5586990714073181, -0.44733601808547974, -0.6192613244056702, -0.9123743772506714, -0.7511847019195557, -0.6118015646934509, -0.6511005163192749, -1.0848991870880127, -0.5875776410102844, -0.9919652342796326, -0.8436933159828186, -0.8645246624946594, -0.8978572487831116, -0.494041383266449, -0.7161688208580017, -1.0434532165527344, -0.5960726737976074, -0.2806702256202698, -0.275179386138916, -1.4085444211959839, -1.3194373846054077, -1.211000919342041, -0.5871408581733704, -0.565078854560852, -0.4973601698875427, -0.6442992091178894, -1.1767538785934448, -0.6408501267433167, -0.5847527980804443, -0.7955131530761719, -0.49460870027542114, -0.867855966091156, -0.3858951926231384, -1.0615922212600708, 0.049148526042699814, -0.6668720245361328, -0.891362726688385, -1.105289340019226, -0.7418306469917297, -0.5061338543891907, -0.4916011691093445, -0.3865751624107361, -1.4276710748672485, -0.4189687967300415, -1.2972171306610107, -1.2275278568267822, -1.1236634254455566, -0.9969573616981506, -1.1425652503967285, -0.938646674156189, -0.8962939381599426, -0.8282960057258606, -0.5431801080703735, -0.7918606400489807, -0.5370401740074158, -0.748617947101593, -0.6436397433280945, -0.5899143815040588, -0.3151590824127197, -1.172228455543518, -1.276859164237976, -0.5147132277488708, -0.6826441884040833, -1.0158631801605225, -0.294080913066864, -0.8161619305610657, -0.9172790050506592, -0.3828984498977661, -0.8566727042198181, -0.4454699754714966, -1.042709231376648, -0.7562350630760193, -0.8325261473655701, -0.1800769567489624, -0.223332017660141, -1.095963478088379, -0.8983249068260193, -0.2861483097076416, -0.7398077249526978, -0.006880644708871841, -1.0623713731765747, -0.5668798685073853, -0.5761584639549255, -1.0276179313659668, -0.7924187183380127, -1.3246798515319824, -0.79583340883255, -0.9070746302604675, -0.41779065132141113, -0.6395956873893738, -0.8150526881217957, -0.7155844569206238, -0.6175970435142517, -0.6883370280265808, -0.9088868498802185, -0.7475898861885071, -0.448138564825058, -0.4586198627948761, -0.9416085481643677, -0.7841959595680237, -0.541954755783081, -0.19083012640476227, -0.742129385471344, -1.2270429134368896, -0.7654969096183777, -0.34826773405075073, -0.9880161285400391, -0.6300901770591736, 0.15596601366996765, -0.7557750940322876, -0.929452657699585, -0.1818924844264984, -0.6969199180603027, -0.1407182216644287, -0.39017659425735474, -0.5124929547309875, -1.2831556797027588, -0.8811997175216675, -0.5988583564758301, -0.7583810687065125, -0.5080025792121887, 0.36809056997299194, -0.6092928051948547, -0.7682299613952637, -0.881187379360199, -0.327551007270813, -0.9285257458686829, -0.6184008717536926, -0.5305103063583374, -0.9858080744743347, -0.5640005469322205, -0.6159953474998474, -1.169703722000122, -0.27942955493927, -1.0850902795791626, -0.5647855401039124, -0.8186978101730347, -0.943020224571228, -0.4241930842399597, -0.7879903316497803, -0.3052065670490265, -0.8964573740959167, -0.510226845741272, -0.6561551690101624, -0.47415363788604736, -0.6560694575309753, -0.5359823703765869, -1.0844849348068237, -0.4666953682899475, -0.3704342842102051, -1.2622560262680054, -0.6261840462684631, -1.0078259706497192, -1.0991488695144653, -0.7480538487434387, -1.062931776046753, -0.2402973473072052, -0.704749345779419, -0.5499748587608337, -0.6634523868560791, -0.17585349082946777, -0.5262444019317627, -1.1008732318878174, -0.741641640663147, -0.8556143641471863, -0.40985724329948425, -1.0737754106521606, -1.1848043203353882, -0.6800026893615723, -0.2955402135848999, -1.0051240921020508, -0.9665771126747131, -0.9785544276237488, -0.8358783721923828, -0.7009491324424744, -0.5138406753540039, 0.1711200475692749, -0.254152774810791, -0.6844218969345093, -0.8313271999359131, -0.43587881326675415, -1.0260082483291626, -0.6342463493347168, -0.060120511800050735, -1.0411773920059204, -0.5749471783638, -1.1336514949798584, -0.7371143698692322, -0.45003050565719604, -0.9301695227622986, -1.0370100736618042, -0.12896722555160522, -0.3530276417732239, -0.6507634520530701, -0.5384917855262756, -0.32560840249061584, -0.43016839027404785, -1.0215365886688232, -1.0578303337097168, -0.6568506956100464, -0.26545286178588867, -0.681993305683136, -0.8565923571586609, -1.33579683303833, -0.9719399809837341, -0.05677323415875435, -0.16111145913600922, -1.2490636110305786, -0.38967961072921753, -1.347561001777649, -0.6767467856407166, -0.4756566286087036, -0.5927582383155823, -0.546890377998352, -0.9271745681762695, -0.5379818081855774, -0.5953906178474426, -0.838676393032074, -1.1014137268066406, -0.774580180644989, -0.91084885597229, -1.115805745124817, -0.9711444973945618, -0.7327154278755188, -0.782026469707489, -1.019871711730957, -1.2052347660064697, -0.021671000868082047, -1.021116852760315, -0.5503507852554321, -0.751774251461029, -1.0557548999786377, -0.34499531984329224, -0.8306726217269897, -0.6809075474739075, -0.9735463261604309, -0.6768589615821838, -0.5932331681251526, -0.6601620316505432, -0.4353160560131073, -0.5308550000190735, -1.1291427612304688, -0.49752095341682434, -0.9679714441299438, -0.786262571811676, -0.5333694219589233, -0.38279616832733154, -0.6788917779922485, -0.7573818564414978, -0.6108667850494385, -0.5848241448402405, -0.731255829334259, -0.8637856245040894, -0.5625489354133606, -0.5013419389724731, -0.9557370543479919, -0.3867475986480713, -0.44954919815063477, -0.5270535349845886, -0.7188072204589844, -0.8740305304527283, -0.833123505115509, -0.40788590908050537, -0.15505251288414001, -0.49684667587280273, -0.6940039992332458, -0.8946213126182556, -1.8306634426116943, -0.5380463600158691, -1.338193416595459, -0.9443662762641907, -0.9171132445335388, -0.5510941743850708, -0.6481309533119202, -0.3380127549171448, -1.259400486946106, -0.6972575783729553, -0.8003141283988953, -0.322049617767334, -0.6443862318992615, -0.4027237296104431, -0.5292808413505554, -0.7227268815040588, -0.7143793702125549, -1.0785640478134155, -0.8529341816902161, -0.25134265422821045, -0.5621750950813293, -0.6283099055290222, -0.5031903982162476, -1.1784073114395142, -1.1029787063598633, -1.1160951852798462, -0.45216888189315796, -0.5824797749519348, -1.3298598527908325, -1.0014922618865967, -0.4635625183582306, -0.5829911828041077, -0.5953215956687927, -0.8216412663459778, -0.2640010416507721, -0.5736348628997803, -1.0677460432052612, -1.110109806060791, -0.8307568430900574, -0.0328301377594471, -0.9230582118034363, -0.5621783137321472, -1.0020864009857178, -0.6105259656906128, -0.2900836169719696, -0.6273362636566162, -0.563827395439148, -0.6404016017913818, -0.6420498490333557, -0.38301175832748413, -0.6769806146621704, -1.152044653892517, -0.9481802582740784, -0.19290295243263245, -1.044019103050232, -0.30523592233657837, 0.018058117479085922, -0.6881260275840759, -0.9934763312339783, -0.7813858389854431, -1.1244523525238037, -0.5721380114555359, -0.3366100788116455, -0.6707541346549988, -1.1765037775039673, -0.7364805936813354, -0.4754578471183777, -0.4790215492248535, -0.6215903162956238, -1.210356593132019, -0.9846968054771423, -0.3962688446044922, -0.7135576009750366, -1.000754714012146, -0.8425715565681458, -0.9773986339569092, -0.4301821291446686, -0.6724496483802795, -0.5447285771369934, -0.6045078039169312, -0.9740356802940369, -0.43558579683303833, -0.5978007912635803, -0.32535579800605774, -0.7171348333358765, -0.5883370637893677, -0.9718226790428162, -0.5181702375411987, -0.33861055970191956, -0.9625609517097473, -0.7327055335044861, -1.2716388702392578, -0.3255383372306824, -0.5538347959518433, -1.4045482873916626, -0.6119500994682312, -1.1701581478118896, -1.0712857246398926, -0.7655047178268433, -0.7494990825653076, -0.8658561110496521, -0.7639387845993042, -0.6292969584465027, -0.8744536638259888, -1.4845370054244995, -0.8605620265007019, -0.7530949115753174, -0.03406253829598427, -0.2264896035194397, -0.12788531184196472, -0.6740776300430298, -1.4299622774124146, -0.25725284218788147, -1.0294902324676514, -0.47501328587532043, -0.753661572933197, -0.7641045451164246, -0.42970895767211914, -1.0069924592971802, -1.0041881799697876, -0.8723543286323547, -0.38500216603279114, -0.6889509558677673, 0.07309511303901672, -0.951770544052124, -0.6090776324272156, -0.21098840236663818, -0.3118581175804138, -0.9137060046195984, -1.1356009244918823, -0.606880247592926, -1.301690936088562, -0.4561118483543396, -0.3552611470222473, -0.8418828248977661, -0.5904918313026428, -0.6140519976615906, -0.033307019621133804, -1.0271553993225098, -0.7644624710083008, -0.5232741832733154, -1.4658806324005127, -0.2536250352859497, -0.9092527031898499, -0.7637905478477478, -0.8520005941390991, -0.5229784250259399, -1.0766124725341797, -0.9359866976737976, -0.5482977032661438, -0.4702482223510742, 0.1443995237350464, -1.2917965650558472, -0.689020574092865, -0.05345432832837105, -0.4690858721733093, -1.1400752067565918, -1.2432963848114014, -0.6962266564369202, -0.3673515021800995, -1.1794371604919434, 0.16283652186393738, -0.8733554482460022, -0.9360020160675049, -0.08786472678184509, 0.01222444698214531, -0.3905539810657501, -0.7441953420639038, -1.0382496118545532, -0.9175830483436584, -0.8932743072509766, -0.5439771413803101, -1.3018429279327393, -0.8593336343765259, -0.7248613834381104, -0.9094341397285461, -0.9233517050743103, -0.690631091594696, -0.12839007377624512, -0.2126120775938034, -1.2658942937850952, -0.8884389996528625, -0.5530356764793396, -0.8733851313591003, -0.7164485454559326, -0.5282077789306641, -0.9069187045097351, -0.4119809865951538, -0.277215838432312, -0.9631162285804749, -0.8532482981681824, -0.7827473282814026, -0.8973767757415771, -0.39778846502304077, -0.9842073917388916, -1.0819203853607178, 0.07416163384914398, -0.4876931607723236, -1.5941927433013916, -0.28923922777175903, -0.463790625333786, -1.2617217302322388, -0.5644943118095398, -0.48058032989501953, -0.71979820728302, -0.9362822771072388, -0.7909480929374695, -0.6527180075645447, -0.18421579897403717, -0.6728164553642273, -0.32343870401382446, -1.067922592163086, -0.42927274107933044, -0.5736686587333679, -0.9085292220115662, -0.7723369002342224, -0.9300277829170227, -0.8912234902381897, -0.4735332727432251, -0.7677260041236877, -1.1785576343536377, -1.1713050603866577, -0.6342992782592773, -0.47533369064331055, -0.5280930995941162, -0.38746029138565063, -0.6170560717582703, -0.4498715400695801, -1.2981258630752563, -0.2866278290748596, -0.630596935749054, -0.09970040619373322, -0.892876386642456, -0.3833121657371521, -0.7789260745048523, -0.8127813935279846, -0.5066322684288025, -0.6570984721183777, -0.11104653775691986, -1.196986436843872, -0.9382142424583435, -0.23235832154750824, -0.800494372844696, -1.0235986709594727, -0.6809320449829102, -0.5145595073699951, -1.0920487642288208, -0.9404725432395935, -0.5966771245002747, -0.266340047121048, -0.9961705803871155, -0.7942441701889038, -0.07238879799842834, -0.6114098429679871, -0.2972569763660431, -0.9646801352500916, -0.6672579646110535, -1.4962317943572998, -0.8394692540168762, -1.2344906330108643, -0.8546289205551147, -0.781315267086029, -0.5379135608673096, -1.0063669681549072, -0.47549980878829956, -0.7122709155082703, -0.8389228582382202, -0.7590463757514954, -0.2688506841659546, -0.6752264499664307, -0.404094398021698, -0.4384952187538147, -0.5739733576774597, -0.3624052405357361, -0.7419938445091248, -0.5735206007957458, -0.9511885046958923, -1.191410779953003, -1.2873636484146118, -0.8828098177909851, -0.5269505381584167, -1.0240856409072876, -0.8505343198776245, -0.49286261200904846, -0.8269288539886475, -1.3509573936462402, -0.5868667960166931, -1.408191204071045, -0.7152081727981567, -0.5939595103263855, -1.1373538970947266, -0.5109164714813232, -1.0789910554885864, -1.211000919342041, -0.8627672791481018, -1.0896267890930176, -1.0318055152893066, -1.2165601253509521, -1.2540652751922607, -0.33269256353378296, -0.7516819834709167, -0.680557131767273, -0.5921993255615234, -0.30899882316589355, -0.9009270071983337, -0.9649099707603455, -0.8112735152244568, -0.46389809250831604, -0.08859771490097046, -0.555043637752533, -0.9130866527557373, -0.38135862350463867, -1.012242078781128, -0.6196652054786682, -0.7135440707206726, -0.6967519521713257, -1.3553221225738525, -0.9874623417854309, -1.120147943496704, -0.026639077812433243, -1.2457164525985718, -0.535934329032898, -0.4816199541091919, -0.8651250004768372, -0.8446095585823059, -0.37563782930374146, -0.7039253115653992, -0.9966503381729126, -0.2603650391101837, -0.854328453540802, -0.1357031613588333, -0.8315584063529968, -1.1301343441009521, -0.8875669836997986, -0.8642012476921082, -0.08396536111831665, -0.5708096623420715, -0.7745565176010132, -1.0708885192871094, -1.1267201900482178, -0.951555609703064, -0.7975262999534607, -0.2712884247303009, -1.179645299911499, -1.0436205863952637, -0.5758726000785828, -0.4391477108001709, -0.9249435663223267, -1.244454264640808, -1.0253865718841553, -0.17607101798057556, -0.33505451679229736, -0.48479509353637695, -0.7774142622947693, -0.8176477551460266, -0.7351664900779724, -1.2976899147033691, -0.5762977600097656, -0.5974942445755005, -0.8782888054847717, -0.28358158469200134, -0.9460800290107727, -1.018511176109314, -0.5963760018348694, -0.9043517708778381, -1.076649785041809, -1.1238735914230347, -0.9308679103851318, -0.12173333764076233, -0.6501167416572571, -0.8227604031562805, -0.9737439751625061, -0.863337516784668, -0.6589741110801697, -0.6353444457054138, -0.7843009829521179, -0.9135263562202454, -0.5756126642227173, -0.18685901165008545, -1.1683952808380127, -0.39527612924575806, -0.6222929358482361, -0.4269554615020752, -0.5188896656036377, -0.38616809248924255, -1.0741794109344482, -0.6453498005867004, -0.8579058647155762, -0.5992786288261414, -0.8435649275779724, -0.7945381999015808, -1.2701027393341064, -0.5438207983970642, -0.7108675837516785, -1.5245592594146729, -0.8944410085678101, -0.730962872505188, -1.1760621070861816, -0.2288370430469513, -0.1418284922838211, -0.9238349199295044, -1.3711613416671753, -0.4523255527019501, -1.0752341747283936, -0.47998425364494324, -0.6617581248283386, -1.282714605331421, -1.0843265056610107, -1.1931053400039673, -0.18796497583389282, -0.7494226098060608, -1.0973927974700928, -0.5228303074836731, -1.065828800201416, -0.7399919629096985, -0.9632387757301331, -0.5904952883720398, -0.9224951863288879, -0.8752282857894897, -0.6545403003692627, -0.9222640991210938, -0.49178415536880493, -0.5251185297966003, -0.6100576519966125, -0.7070443630218506, -0.72805255651474, -0.5976554155349731, -0.33926963806152344, -0.49038636684417725, -0.9425459504127502, -0.7177327871322632, -0.8631070852279663, -1.0795820951461792, -1.4210726022720337, -0.5735167264938354, -0.33512455224990845, -1.2222514152526855, -1.1112455129623413, -1.2139227390289307, -0.2733400762081146, -0.9172156453132629, -0.5824507474899292, -1.0747592449188232, -0.43320444226264954, -0.7284086346626282, -0.30794060230255127, -1.1787145137786865, -0.9650971293449402, -0.8255044221878052, -1.0299280881881714, -0.5205990672111511, -0.7222949862480164, -0.7171546220779419, -0.356866717338562, -0.3547884225845337, -1.2311128377914429, -1.0091838836669922, -0.22042813897132874, -0.4883037209510803, -0.779843270778656, -0.6698072552680969, -0.5659344792366028, -0.36030781269073486, -0.8081835508346558, -0.6787497401237488, -0.8094916939735413, -0.8169005513191223, -0.8046227693557739, -0.002817738801240921, -0.874452531337738, -0.04678294435143471, -0.22932566702365875, -1.000160574913025, -0.7249985933303833, -0.11674746870994568, -0.7659263014793396, -0.6286913752555847, -1.2723677158355713, -0.4384478032588959, -0.16170422732830048, -1.0638699531555176, -0.6151407361030579, -0.4356340765953064, -0.5624446272850037, -0.5684419870376587, -0.32765257358551025, -0.6715850234031677, -1.248935580253601, -0.7508839964866638, -0.5669087767601013, -0.5628349781036377, -0.8177953958511353, -0.4267561137676239, -0.21289852261543274, -0.35562556982040405, -1.1805144548416138, -0.8336929082870483, -0.578657865524292, -0.7349891066551208, -1.4271503686904907, -0.7717013955116272, -0.7632899284362793, -0.6667603850364685, -0.9055008292198181, -0.29562950134277344, -0.8026874661445618, -1.053182601928711, 0.1065576821565628, -0.5483530759811401]}\n",
            "Avg BLEURT Score: -0.7280065919678587\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7cd642a5727c4146ac475667867c607e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8a431b75ac8424f8734580e045e57aa",
              "IPY_MODEL_0bd485803a00419ca0fd7efc5ff63bac",
              "IPY_MODEL_d224f4fce71f4ebeb7c25f4617bac786"
            ],
            "layout": "IPY_MODEL_0681e895ea874b3082d03538534ebdfe"
          }
        },
        "e8a431b75ac8424f8734580e045e57aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a85fa24b77be493fa1a3e670f974923c",
            "placeholder": "​",
            "style": "IPY_MODEL_cf5df40390284c6a8148d57f94e6f5c0",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "0bd485803a00419ca0fd7efc5ff63bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d747fbfee5f84a00a8adb523c86a9acc",
            "max": 2021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_524772da150f4b1ca419ac5075f72ca9",
            "value": 2021
          }
        },
        "d224f4fce71f4ebeb7c25f4617bac786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a19041d8f604a688c1c5c7d60b4ed9f",
            "placeholder": "​",
            "style": "IPY_MODEL_c502f48fdc2a496dbe6b268aa3b03bad",
            "value": " 2021/2021 [00:09&lt;00:00, 332.11 examples/s]"
          }
        },
        "0681e895ea874b3082d03538534ebdfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a85fa24b77be493fa1a3e670f974923c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf5df40390284c6a8148d57f94e6f5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d747fbfee5f84a00a8adb523c86a9acc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "524772da150f4b1ca419ac5075f72ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a19041d8f604a688c1c5c7d60b4ed9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c502f48fdc2a496dbe6b268aa3b03bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c54dcf63195d4dd48eba8f05fbe7510d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8abff75537249ef8ec7a8204754a4be",
              "IPY_MODEL_5a00dfea9e2546b18eb5accb262f441f",
              "IPY_MODEL_35b026e73ceb4283aca19d65d03d1847"
            ],
            "layout": "IPY_MODEL_be65e2f5d4924b0eaccdab4c234f07eb"
          }
        },
        "f8abff75537249ef8ec7a8204754a4be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d0a1221e1234e29b65e251849ffc73b",
            "placeholder": "​",
            "style": "IPY_MODEL_41919fd5f31743a9957e5633af00100f",
            "value": "config.json: 100%"
          }
        },
        "5a00dfea9e2546b18eb5accb262f441f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28c24f0f162144fea396a1d8e5575aaf",
            "max": 1054,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da1e32028fdc427eaa669c6553931864",
            "value": 1054
          }
        },
        "35b026e73ceb4283aca19d65d03d1847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfefc6d842bc4501b12eada6c4241251",
            "placeholder": "​",
            "style": "IPY_MODEL_4d92e56a732e476ebb1dc487a133d2df",
            "value": " 1.05k/1.05k [00:00&lt;00:00, 62.2kB/s]"
          }
        },
        "be65e2f5d4924b0eaccdab4c234f07eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d0a1221e1234e29b65e251849ffc73b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41919fd5f31743a9957e5633af00100f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28c24f0f162144fea396a1d8e5575aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da1e32028fdc427eaa669c6553931864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfefc6d842bc4501b12eada6c4241251": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d92e56a732e476ebb1dc487a133d2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ca4afc56c0841d4b0437a2d3763f23c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8f625e0010c4d4886c0c140cd23b2d3",
              "IPY_MODEL_360a78fb5fdb4a0194f10c190b6ede65",
              "IPY_MODEL_fb4d7ce9a5664ab2b176068b7a405d7a"
            ],
            "layout": "IPY_MODEL_d5d23724302649f7a4650c85633f0b05"
          }
        },
        "d8f625e0010c4d4886c0c140cd23b2d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c24a68d4d99545cb9fde7f5630ef73b6",
            "placeholder": "​",
            "style": "IPY_MODEL_a438698514af4d15aee04334cb3aaa4b",
            "value": "model.safetensors: 100%"
          }
        },
        "360a78fb5fdb4a0194f10c190b6ede65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cacb019b8664ff9bf209b4f83029f7d",
            "max": 990386200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_049e01a80cbb45b392c38e5be899d0db",
            "value": 990386200
          }
        },
        "fb4d7ce9a5664ab2b176068b7a405d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8362f0352eec4c6f9a56e4c6849fa656",
            "placeholder": "​",
            "style": "IPY_MODEL_59275ff7ebbd44fbbafdbc91fdcea170",
            "value": " 990M/990M [01:18&lt;00:00, 15.8MB/s]"
          }
        },
        "d5d23724302649f7a4650c85633f0b05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c24a68d4d99545cb9fde7f5630ef73b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a438698514af4d15aee04334cb3aaa4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cacb019b8664ff9bf209b4f83029f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "049e01a80cbb45b392c38e5be899d0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8362f0352eec4c6f9a56e4c6849fa656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59275ff7ebbd44fbbafdbc91fdcea170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "369ef19db1bd4945822b0bbd38cfb7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c60a26c03b34238a9c7605f7ff02110",
              "IPY_MODEL_3373825265914df1afbea22c1e754dbc",
              "IPY_MODEL_b8d2bed75aaa4fc0a56a25f9c7d353ad"
            ],
            "layout": "IPY_MODEL_6e02a617f40b4d1bbcc4f8932c48aa86"
          }
        },
        "8c60a26c03b34238a9c7605f7ff02110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6cf248920f54f7a95291f6dae68f2e5",
            "placeholder": "​",
            "style": "IPY_MODEL_233e4ebe9fa54e5789cea99738e14415",
            "value": "Downloading data: 100%"
          }
        },
        "3373825265914df1afbea22c1e754dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f98692d7f9e24462b0060bc194c7b884",
            "max": 259623333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfc7d85bbcd14adfb5fc1eaec076dff8",
            "value": 259623333
          }
        },
        "b8d2bed75aaa4fc0a56a25f9c7d353ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d59285901b4109b15ff126d8e17367",
            "placeholder": "​",
            "style": "IPY_MODEL_2d645065d82d4d81ac13895a32518499",
            "value": " 260M/260M [00:11&lt;00:00, 27.1MB/s]"
          }
        },
        "6e02a617f40b4d1bbcc4f8932c48aa86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6cf248920f54f7a95291f6dae68f2e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "233e4ebe9fa54e5789cea99738e14415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f98692d7f9e24462b0060bc194c7b884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfc7d85bbcd14adfb5fc1eaec076dff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81d59285901b4109b15ff126d8e17367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d645065d82d4d81ac13895a32518499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fce0ee2e40e846b18175bb5c1fcd7fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_add4683fd03a4bdfad7963f75e2e922e",
              "IPY_MODEL_a27b6081b0934dd49d1cf45427cf540d",
              "IPY_MODEL_674aaa178ab840bd99c7dc65054a8795"
            ],
            "layout": "IPY_MODEL_fe622282673e4bf6a1fbd38574ada83e"
          }
        },
        "add4683fd03a4bdfad7963f75e2e922e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f09fc1ebb6e64fc0abd0923175ac71e3",
            "placeholder": "​",
            "style": "IPY_MODEL_6fc4465b344b42e892eaaf9f87cac76d",
            "value": "Downloading data: 100%"
          }
        },
        "a27b6081b0934dd49d1cf45427cf540d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48614291d2224aceb738df56496a6134",
            "max": 48885963,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ba9e8021d744cd99ced2bb91aa4ae55",
            "value": 48885963
          }
        },
        "674aaa178ab840bd99c7dc65054a8795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_616847af91dd41d7b7512ef5f2b64309",
            "placeholder": "​",
            "style": "IPY_MODEL_444076d2e6a34048b6566a672c53b35e",
            "value": " 48.9M/48.9M [00:07&lt;00:00, 7.34MB/s]"
          }
        },
        "fe622282673e4bf6a1fbd38574ada83e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f09fc1ebb6e64fc0abd0923175ac71e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fc4465b344b42e892eaaf9f87cac76d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48614291d2224aceb738df56496a6134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ba9e8021d744cd99ced2bb91aa4ae55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "616847af91dd41d7b7512ef5f2b64309": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "444076d2e6a34048b6566a672c53b35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17622eb4b5c5401eba08644ee30eb1ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_665df60c49b7457f934180e6903f0a69",
              "IPY_MODEL_e89b3d3ea574470193e2101ef958d059",
              "IPY_MODEL_fc4cd4d23622499abbcd30189044eb04"
            ],
            "layout": "IPY_MODEL_4327990ac9c842a1a2a0a6d2b64c608d"
          }
        },
        "665df60c49b7457f934180e6903f0a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_924c6ae76f1a434fa7a1b711698856b8",
            "placeholder": "​",
            "style": "IPY_MODEL_9cc77d2db4d147a2b744d238bc588028",
            "value": "Downloading data: 100%"
          }
        },
        "e89b3d3ea574470193e2101ef958d059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0858dfd519254007a1a013c20ada670c",
            "max": 39897357,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c42af913fdb41e3ba6fe7cfeee106f7",
            "value": 39897357
          }
        },
        "fc4cd4d23622499abbcd30189044eb04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_862927a54827468fa423b10814be047b",
            "placeholder": "​",
            "style": "IPY_MODEL_d5a67f24ba75401a8f9567d39ac248c7",
            "value": " 39.9M/39.9M [00:05&lt;00:00, 7.57MB/s]"
          }
        },
        "4327990ac9c842a1a2a0a6d2b64c608d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "924c6ae76f1a434fa7a1b711698856b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cc77d2db4d147a2b744d238bc588028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0858dfd519254007a1a013c20ada670c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c42af913fdb41e3ba6fe7cfeee106f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "862927a54827468fa423b10814be047b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5a67f24ba75401a8f9567d39ac248c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66f2b49f3a454b0a9b0c91d5c2eedff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4813d4a7f75c4594b7839e51208e0a54",
              "IPY_MODEL_3a3ac1325458414b9767b6b1048a0323",
              "IPY_MODEL_4115d45877254543ad331bd29e064fc2"
            ],
            "layout": "IPY_MODEL_40ad83c157264f128a30e9111979ba77"
          }
        },
        "4813d4a7f75c4594b7839e51208e0a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73f1884768fe416ab6705aa0874cba9f",
            "placeholder": "​",
            "style": "IPY_MODEL_6ddeb287e9c54406882ae0511fb655fa",
            "value": "Downloading data: 100%"
          }
        },
        "3a3ac1325458414b9767b6b1048a0323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55ae228d569848c8b80f4bf9e9550062",
            "max": 46731628,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09cefc5ab6f0410d8ffcb18323aeca0f",
            "value": 46731628
          }
        },
        "4115d45877254543ad331bd29e064fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e5f32107e664210a8b407b542189341",
            "placeholder": "​",
            "style": "IPY_MODEL_599fc65c032343b09b246c1dcf440110",
            "value": " 46.7M/46.7M [00:21&lt;00:00, 2.37MB/s]"
          }
        },
        "40ad83c157264f128a30e9111979ba77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73f1884768fe416ab6705aa0874cba9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ddeb287e9c54406882ae0511fb655fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55ae228d569848c8b80f4bf9e9550062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09cefc5ab6f0410d8ffcb18323aeca0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e5f32107e664210a8b407b542189341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "599fc65c032343b09b246c1dcf440110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76caa49180534417bda0f230f7ee693e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54cff3954c2f4f3a8420a48e63189c1a",
              "IPY_MODEL_10ae6e9d200548de900b80c948d73eed",
              "IPY_MODEL_da7cdf365a4844129a5ef3d9752b1be1"
            ],
            "layout": "IPY_MODEL_cbf90dfe961b408d9922f1caba2184a5"
          }
        },
        "54cff3954c2f4f3a8420a48e63189c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_817d5de9a47245d296ac1cc6b228c2ae",
            "placeholder": "​",
            "style": "IPY_MODEL_395355072bd94afaa9c35969e7269d66",
            "value": "Generating train split: 100%"
          }
        },
        "10ae6e9d200548de900b80c948d73eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_390cc68c80e441d89aa959303f946ed6",
            "max": 14188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2c56cf289db4b3d99e94d15e13af5fd",
            "value": 14188
          }
        },
        "da7cdf365a4844129a5ef3d9752b1be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46f4b5ee319e4f049198e8d694adf15f",
            "placeholder": "​",
            "style": "IPY_MODEL_7bd30cf43aa74814bbffe05921b7e2b1",
            "value": " 14188/14188 [00:02&lt;00:00, 6713.62 examples/s]"
          }
        },
        "cbf90dfe961b408d9922f1caba2184a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817d5de9a47245d296ac1cc6b228c2ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "395355072bd94afaa9c35969e7269d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "390cc68c80e441d89aa959303f946ed6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2c56cf289db4b3d99e94d15e13af5fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46f4b5ee319e4f049198e8d694adf15f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bd30cf43aa74814bbffe05921b7e2b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea3510fc5d0546c39f34cec6b6d4abfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bffed69e2014102a0917dacd3ce8f85",
              "IPY_MODEL_bbadd3e25ead4466931dd5e4bb778346",
              "IPY_MODEL_aaa4ef4e134449be9029cdee17bd034e"
            ],
            "layout": "IPY_MODEL_f07486d4226b46ed9b4f6c893a9d89d5"
          }
        },
        "5bffed69e2014102a0917dacd3ce8f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18498703326a4fd68d421ef4e3d5f76c",
            "placeholder": "​",
            "style": "IPY_MODEL_780237a621d04301b20ca1ba7d96e351",
            "value": "Generating test split: 100%"
          }
        },
        "bbadd3e25ead4466931dd5e4bb778346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_877b7ac073a9425896b05b9713c86ce7",
            "max": 1667,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a43d63e319be407fa1acde1fa9fbf250",
            "value": 1667
          }
        },
        "aaa4ef4e134449be9029cdee17bd034e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9862ef3fb0f04166a90c6274e42e6278",
            "placeholder": "​",
            "style": "IPY_MODEL_72a15c9ab3a34320bf10fa041f781664",
            "value": " 1667/1667 [00:00&lt;00:00, 5783.03 examples/s]"
          }
        },
        "f07486d4226b46ed9b4f6c893a9d89d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18498703326a4fd68d421ef4e3d5f76c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "780237a621d04301b20ca1ba7d96e351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "877b7ac073a9425896b05b9713c86ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a43d63e319be407fa1acde1fa9fbf250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9862ef3fb0f04166a90c6274e42e6278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72a15c9ab3a34320bf10fa041f781664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e814e66070714b5fba3bdaffd7e798e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9d001cd16ae433286546124913439f3",
              "IPY_MODEL_a327fb33737d445b8b68ecd5c36fd6ad",
              "IPY_MODEL_34edcabed7974517a7176983bf5c82b0"
            ],
            "layout": "IPY_MODEL_60a9b8fa2a1c4ec6b22b376dcd778aa6"
          }
        },
        "f9d001cd16ae433286546124913439f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40004c7051214066a28759e88723a833",
            "placeholder": "​",
            "style": "IPY_MODEL_6f78453bbbe7493ca8a5a4ee8ec3e2dc",
            "value": "Generating validation split: 100%"
          }
        },
        "a327fb33737d445b8b68ecd5c36fd6ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7253d956bfc9422d9eb75c1b9651a2ac",
            "max": 2021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb00e9944d3e465da92c3ec14d9c6428",
            "value": 2021
          }
        },
        "34edcabed7974517a7176983bf5c82b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebdf7978f15f43199a20378bbe15bded",
            "placeholder": "​",
            "style": "IPY_MODEL_61e9e21972774207bf1e511b87837b94",
            "value": " 2021/2021 [00:00&lt;00:00, 5535.32 examples/s]"
          }
        },
        "60a9b8fa2a1c4ec6b22b376dcd778aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40004c7051214066a28759e88723a833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f78453bbbe7493ca8a5a4ee8ec3e2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7253d956bfc9422d9eb75c1b9651a2ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb00e9944d3e465da92c3ec14d9c6428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebdf7978f15f43199a20378bbe15bded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61e9e21972774207bf1e511b87837b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}